diff --git a/.coveragerc b/.coveragerc
new file mode 100644
index 0000000..b3e9e3a
--- /dev/null
+++ b/.coveragerc
@@ -0,0 +1,23 @@
+[run]
+source = src/fapilog
+omit = 
+    examples/*
+    tests/*
+    */tests/*
+    */test_*
+    *_test.py
+    setup.py
+    conftest.py
+
+[report]
+exclude_lines =
+    pragma: no cover
+    def __repr__
+    if self.debug:
+    if settings.DEBUG
+    raise AssertionError
+    raise NotImplementedError
+    if 0:
+    if __name__ == .__main__.:
+    class .*\bProtocol\):
+    @(abc\.)?abstractmethod 
\ No newline at end of file
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
new file mode 100644
index 0000000..adcabbf
--- /dev/null
+++ b/CONTRIBUTING.md
@@ -0,0 +1,219 @@
+# Contributing to fapilog
+
+First, thank you for considering contributing to **fapilog**! Your help is appreciated, whether it's filing an issue, improving documentation, or submitting code.
+
+---
+
+## üìú Code of Conduct
+
+We follow the [Contributor Covenant Code of Conduct](https://www.contributor-covenant.org/version/2/1/code_of_conduct/). Please be respectful and constructive in all interactions.
+
+---
+
+## üí° How to Contribute
+
+You can contribute in many ways:
+
+- File bug reports and feature requests via [GitHub Issues](https://github.com/fapilog/fapilog/issues)
+- Improve or clarify the documentation
+- Submit bug fixes or new features via Pull Request (PR)
+- Add tests for new or existing features
+- Improve the async logging queue performance
+- Add new sinks (Loki, HTTP endpoints, etc.)
+- Enhance the middleware or enrichers
+
+---
+
+## ‚öôÔ∏è Development Setup
+
+### 1. Fork and Clone
+
+```bash
+git clone https://github.com/your-username/fapilog.git
+cd fapilog
+```
+
+### 2. Create a Virtual Environment
+
+```bash
+python -m venv .venv
+source .venv/bin/activate  # On Windows: .venv\Scripts\activate
+```
+
+### 3. Install Dependencies
+
+```bash
+pip install -e ".[dev]"
+```
+
+This installs the package in editable mode with all development dependencies including:
+
+- `pytest` and `pytest-asyncio` for testing
+- `ruff` for linting and formatting
+- `mypy` for type checking
+- `pytest-cov` for coverage reporting
+- `fastapi` and `httpx` for integration tests
+
+### 4. Run the Tests
+
+```bash
+hatch run test
+```
+
+This runs the full test suite with coverage reporting. The project enforces a minimum coverage threshold of 90%.
+
+### 5. Format and Lint
+
+We use [`ruff`](https://docs.astral.sh/ruff/) for both linting and formatting:
+
+```bash
+hatch run lint
+```
+
+Or run formatting and linting separately:
+
+```bash
+ruff format .
+ruff check .
+```
+
+### 6. Type Checking
+
+```bash
+hatch run typecheck
+```
+
+---
+
+## üß™ Testing
+
+### Running Tests
+
+- **Full test suite**: `hatch run test`
+- **Coverage report**: `hatch run test-cov`
+- **Load testing**: `hatch run test-queue-load`
+
+### Test Coverage
+
+The project enforces a **90% minimum coverage threshold**. Pull requests must maintain or improve test coverage. Coverage reports are generated in `htmlcov/` after running tests.
+
+### Test Structure
+
+- **Unit tests**: `tests/test_*.py`
+- **Integration tests**: Marked with `@pytest.mark.integration`
+- **Async tests**: Marked with `@pytest.mark.asyncio`
+- **Slow tests**: Marked with `@pytest.mark.slow`
+
+---
+
+## üèóÔ∏è Project Structure
+
+```
+src/fapilog/
+‚îú‚îÄ‚îÄ __init__.py          # Public API
+‚îú‚îÄ‚îÄ bootstrap.py         # Logging configuration
+‚îú‚îÄ‚îÄ middleware.py        # FastAPI middleware
+‚îú‚îÄ‚îÄ enrichers.py        # Log enrichment
+‚îú‚îÄ‚îÄ pipeline.py          # Logging pipeline
+‚îú‚îÄ‚îÄ settings.py          # Configuration
+‚îú‚îÄ‚îÄ _internal/          # Internal utilities
+‚îÇ   ‚îú‚îÄ‚îÄ context.py      # Context management
+‚îÇ   ‚îú‚îÄ‚îÄ queue.py        # Async queue
+‚îÇ   ‚îî‚îÄ‚îÄ utils.py        # Utilities
+‚îî‚îÄ‚îÄ sinks/              # Log output handlers
+    ‚îú‚îÄ‚îÄ __init__.py
+    ‚îú‚îÄ‚îÄ stdout.py       # Console output
+    ‚îî‚îÄ‚îÄ loki.py         # Loki integration
+```
+
+### Key Components
+
+- **Middleware**: `TraceIDMiddleware` for request tracing
+- **Queue**: Async, non-blocking log processing
+- **Sinks**: Pluggable output handlers (stdout, Loki, etc.)
+- **Enrichers**: Context-aware log enhancement
+- **Settings**: Pydantic-based configuration
+
+---
+
+## ‚úÖ Submitting a Pull Request
+
+1. Create a new branch from `main`:
+
+   ```bash
+   git checkout -b feature/my-awesome-feature
+   ```
+
+2. Follow existing code style and add appropriate tests.
+
+3. Run tests and lint before submitting:
+
+   ```bash
+   hatch run test
+   hatch run lint
+   hatch run typecheck
+   ```
+
+4. Push to your fork and open a PR with a **clear description** of your changes and motivation.
+
+> We squash and merge PRs. Keep commits clean and meaningful.
+
+---
+
+## üîß Development Commands
+
+| Command                     | Description                      |
+| --------------------------- | -------------------------------- |
+| `hatch run lint`            | Run Ruff linter and formatter    |
+| `hatch run typecheck`       | Run MyPy type checker            |
+| `hatch run test`            | Run pytest with coverage         |
+| `hatch run test-cov`        | Run tests with detailed coverage |
+| `hatch run test-queue-load` | Run async queue load testing     |
+
+---
+
+## üß± Dependencies
+
+### Core Dependencies
+
+- `structlog` - Structured logging (required)
+- `pydantic>=2.0.0` - Settings and validation
+- `pydantic-settings>=2.0.0` - Configuration management
+- `anyio` - Async utilities
+
+### Optional Dependencies
+
+- `fastapi>=0.100.0` - For FastAPI integration
+- `httpx>=0.27.0` - For HTTP sinks and testing
+- `psutil>=5.9` - For resource metrics
+
+### Development Dependencies
+
+- `pytest>=7.0.0` - Testing framework
+- `pytest-asyncio>=0.21.0` - Async test support
+- `ruff>=0.1.0` - Linting and formatting
+- `mypy>=1.0.0` - Type checking
+- `pytest-cov>=4.0.0` - Coverage reporting
+
+**Note**: Your code **may not introduce unnecessary dependencies**. All new dependencies must be justified and approved.
+
+---
+
+## üöÄ Performance Considerations
+
+When contributing to fapilog, consider the performance impact:
+
+- **Async-first**: All I/O operations should be non-blocking
+- **Queue efficiency**: The async queue is critical for performance
+- **Memory usage**: Avoid unbounded memory growth
+- **Context propagation**: Use `contextvars` for request-scoped data
+
+---
+
+## üìù License
+
+By contributing, you agree that your contributions will be licensed under the MIT License, the same as the rest of the project.
+
+---
+
+Thanks again for helping improve **fapilog**!
diff --git a/docs/api-reference.md b/docs/api-reference.md
new file mode 100644
index 0000000..68ab6e2
--- /dev/null
+++ b/docs/api-reference.md
@@ -0,0 +1,715 @@
+# API Reference
+
+This document provides a complete reference for all public APIs in `fapilog`. The API is designed to be simple for basic usage while remaining fully extensible for advanced scenarios.
+
+---
+
+## Table of Contents
+
+- [Core Functions](#core-functions)
+- [Configuration](#configuration)
+- [Logging Interface](#logging-interface)
+- [Middleware](#middleware)
+- [Enrichers](#enrichers)
+- [Sinks](#sinks)
+- [Context Management](#context-management)
+- [Types and Models](#types-and-models)
+
+---
+
+## Core Functions
+
+### `configure_logging()`
+
+The primary function for setting up structured logging in your application.
+
+```python
+from fapilog import configure_logging
+
+# Basic usage with defaults
+configure_logging()
+
+# With FastAPI app (automatically adds middleware)
+from fastapi import FastAPI
+app = FastAPI()
+configure_logging(app=app)
+
+# With custom settings
+from fapilog.settings import LoggingSettings
+settings = LoggingSettings(level="DEBUG", queue_enabled=True)
+configure_logging(settings=settings)
+```
+
+**Parameters:**
+
+- `level` (str, optional): Logging level override. Defaults to `None` (uses settings).
+- `sinks` (Dict[str, Any], optional): Sink configurations. **Deprecated** - use `LoggingSettings` instead.
+- `json_console` (str, optional): Console output format override. **Deprecated** - use `LoggingSettings` instead.
+- `settings` (LoggingSettings, optional): Complete configuration object. If `None`, created from environment variables.
+- `app` (Any, optional): FastAPI app instance. If provided, `TraceIDMiddleware` is automatically registered.
+
+**Returns:**
+
+- `structlog.BoundLogger`: Configured logger instance
+
+**Raises:**
+
+- `RuntimeError`: If called from an async context without proper setup
+
+**Notes:**
+
+- This function is idempotent - subsequent calls will not duplicate handlers
+- When `app` is provided, middleware is registered and shutdown handlers are configured
+- Environment variables with `FAPILOG_` prefix are automatically loaded
+
+### `reset_logging()`
+
+Reset logging configuration for testing purposes.
+
+```python
+from fapilog import reset_logging
+
+# Reset all configuration
+reset_logging()
+```
+
+**Use Cases:**
+
+- Unit testing to ensure clean state between tests
+- Development environments where you need to reconfigure logging
+- Debugging configuration issues
+
+---
+
+## Configuration
+
+### `LoggingSettings`
+
+Pydantic-based configuration model that maps environment variables to logging behavior.
+
+```python
+from fapilog.settings import LoggingSettings
+
+# Create from environment variables (default)
+settings = LoggingSettings()
+
+# Create with custom values
+settings = LoggingSettings(
+    level="DEBUG",
+    sinks=["stdout", "loki"],
+    json_console="pretty",
+    queue_enabled=True,
+    queue_maxsize=2000,
+    sampling_rate=0.5
+)
+```
+
+**Fields:**
+
+| Field                     | Type      | Default      | Description                                        |
+| ------------------------- | --------- | ------------ | -------------------------------------------------- |
+| `level`                   | str       | `"INFO"`     | Logging level (DEBUG, INFO, WARN, ERROR, CRITICAL) |
+| `sinks`                   | List[str] | `["stdout"]` | List of sink names for log output                  |
+| `json_console`            | str       | `"auto"`     | Console format (auto, json, pretty)                |
+| `redact_patterns`         | List[str] | `[]`         | Regex patterns to redact from logs                 |
+| `sampling_rate`           | float     | `1.0`        | Log sampling rate (0.0 to 1.0)                     |
+| `queue_enabled`           | bool      | `True`       | Enable async queue for non-blocking logging        |
+| `queue_maxsize`           | int       | `1000`       | Maximum size of async log queue                    |
+| `queue_overflow`          | str       | `"drop"`     | Queue overflow strategy (drop, block, sample)      |
+| `queue_batch_size`        | int       | `10`         | Events per batch                                   |
+| `queue_batch_timeout`     | float     | `1.0`        | Batch timeout in seconds                           |
+| `queue_retry_delay`       | float     | `1.0`        | Retry delay in seconds                             |
+| `queue_max_retries`       | int       | `3`          | Maximum retries per event                          |
+| `enable_resource_metrics` | bool      | `False`      | Enable memory/CPU metrics in logs                  |
+
+**Environment Variables:**
+
+All fields can be configured via environment variables with the `FAPILOG_` prefix:
+
+```bash
+export FAPILOG_LEVEL=DEBUG
+export FAPILOG_QUEUE_ENABLED=true
+export FAPILOG_QUEUE_MAXSIZE=2000
+export FAPILOG_SINKS=stdout,loki
+```
+
+---
+
+## Logging Interface
+
+### `log`
+
+The main logger instance that provides structured logging capabilities.
+
+```python
+from fapilog import log
+
+# Basic logging
+log.info("User logged in")
+log.error("Database connection failed")
+
+# Structured logging with key-value pairs
+log.info("User action", user_id="123", action="login", ip="192.168.1.1")
+log.error("API error", status_code=500, endpoint="/api/users", error="timeout")
+
+# Logging with context
+log.warning("High memory usage", memory_mb=1024, threshold=512)
+log.debug("Processing request", request_id="abc123", duration_ms=45.2)
+```
+
+**Available Methods:**
+
+- `log.debug(message, **kwargs)` - Debug level logging
+- `log.info(message, **kwargs)` - Info level logging
+- `log.warning(message, **kwargs)` - Warning level logging
+- `log.error(message, **kwargs)` - Error level logging
+- `log.critical(message, **kwargs)` - Critical level logging
+
+**Features:**
+
+- **Structured Output**: All logs are JSON objects in production
+- **Context Enrichment**: Automatically includes trace_id, hostname, PID, etc.
+- **Async Safe**: Non-blocking logging via background queue
+- **Pretty Console**: Human-readable output in development
+
+**Example Output:**
+
+```json
+{
+  "timestamp": "2024-01-15T10:30:45.123Z",
+  "level": "info",
+  "event": "User logged in",
+  "trace_id": "abc123def456",
+  "span_id": "789ghi012jkl",
+  "hostname": "server-01",
+  "pid": 12345,
+  "user_id": "123",
+  "action": "login",
+  "ip": "192.168.1.1"
+}
+```
+
+---
+
+## Middleware
+
+### `TraceIDMiddleware`
+
+FastAPI middleware that provides request correlation and timing.
+
+```python
+from fapilog.middleware import TraceIDMiddleware
+from fastapi import FastAPI
+
+app = FastAPI()
+
+# Add middleware manually
+app.add_middleware(TraceIDMiddleware)
+
+# Or automatically via configure_logging
+from fapilog import configure_logging
+configure_logging(app=app)
+```
+
+**Features:**
+
+- **Trace Correlation**: Generates or forwards trace_id from `X-Trace-Id` header
+- **Span Generation**: Creates unique span_id for each request
+- **Request Timing**: Measures and logs request latency
+- **Response Headers**: Adds correlation headers to responses
+- **Context Isolation**: Ensures clean context between requests
+
+**Response Headers:**
+
+- `X-Trace-Id`: Request trace identifier
+- `X-Span-Id`: Request span identifier
+- `X-Response-Time-ms`: Request latency in milliseconds
+
+**Captured Metadata:**
+
+- Request path and method
+- HTTP status code
+- Request/response body sizes
+- User-Agent header
+- Request latency
+- Error details (if applicable)
+
+### `add_trace_exception_handler()`
+
+Register a custom exception handler that adds trace headers to error responses.
+
+```python
+from fapilog.middleware import add_trace_exception_handler
+from fastapi import FastAPI
+
+app = FastAPI()
+add_trace_exception_handler(app)
+```
+
+**Features:**
+
+- Adds trace headers to 500 error responses
+- Preserves original error handling
+- Includes latency information in error responses
+
+---
+
+## Enrichers
+
+Enrichers add metadata to log events automatically. Built-in enrichers provide system and request information.
+
+### Built-in Enrichers
+
+#### `host_process_enricher()`
+
+Adds hostname and process ID to all log events.
+
+```python
+# Automatically applied - no manual configuration needed
+```
+
+**Added Fields:**
+
+- `hostname`: System hostname
+- `pid`: Process ID
+
+#### `resource_snapshot_enricher()`
+
+Adds memory and CPU usage metrics to log events.
+
+```python
+# Enable via settings
+settings = LoggingSettings(enable_resource_metrics=True)
+configure_logging(settings=settings)
+```
+
+**Added Fields:**
+
+- `memory_mb`: Resident memory usage in MB
+- `cpu_percent`: Process CPU usage percentage
+
+#### `body_size_enricher()`
+
+Adds request and response body sizes to log events.
+
+```python
+# Automatically applied when TraceIDMiddleware is used
+```
+
+**Added Fields:**
+
+- `req_bytes`: Request body size in bytes
+- `res_bytes`: Response body size in bytes
+
+#### `request_response_enricher()`
+
+Adds comprehensive request/response metadata to log events.
+
+```python
+# Automatically applied when TraceIDMiddleware is used
+```
+
+**Added Fields:**
+
+- `status_code`: HTTP status code
+- `latency_ms`: Request latency in milliseconds
+- `req_bytes`: Request body size
+- `res_bytes`: Response body size
+- `user_agent`: User-Agent header value
+
+### Custom Enrichers
+
+Register custom enrichers to add application-specific metadata.
+
+```python
+from fapilog.enrichers import register_enricher
+
+@register_enricher
+def user_context_enricher(logger, method_name, event_dict):
+    """Add user context from session."""
+    # Get user from your session/context
+    user = get_current_user()
+    if user:
+        event_dict["user_id"] = user.id
+        event_dict["user_role"] = user.role
+    return event_dict
+
+@register_enricher
+def application_enricher(logger, method_name, event_dict):
+    """Add application-specific metadata."""
+    event_dict["app_version"] = "1.2.3"
+    event_dict["environment"] = "production"
+    return event_dict
+```
+
+**Enricher Function Signature:**
+
+```python
+def enricher_function(logger, method_name, event_dict):
+    """
+    Args:
+        logger: The logger instance
+        method_name: The logging method name (info, error, etc.)
+        event_dict: The event dictionary to enrich
+
+    Returns:
+        The enriched event dictionary
+    """
+    # Add your fields to event_dict
+    event_dict["custom_field"] = "value"
+    return event_dict
+```
+
+### Enricher Management
+
+```python
+from fapilog.enrichers import register_enricher, clear_enrichers
+
+# Register multiple enrichers
+register_enricher(custom_enricher_1)
+register_enricher(custom_enricher_2)
+
+# Clear all custom enrichers (for testing)
+clear_enrichers()
+```
+
+---
+
+## Sinks
+
+Sinks handle writing log events to different destinations. Built-in sinks provide common output targets.
+
+### Built-in Sinks
+
+#### `StdoutSink`
+
+Writes log events to stdout with JSON or pretty formatting.
+
+```python
+from fapilog.sinks.stdout import StdoutSink
+
+# JSON output (production)
+sink = StdoutSink(pretty=False)
+
+# Pretty output (development)
+sink = StdoutSink(pretty=True)
+```
+
+**Features:**
+
+- JSON or pretty console output
+- Automatic flushing
+- Thread-safe writing
+- Configurable via `LoggingSettings.json_console`
+
+### Custom Sinks
+
+Create custom sinks for specific output destinations.
+
+```python
+from fapilog._internal.queue import Sink
+import httpx
+
+class HTTPApiSink(Sink):
+    """Send logs to HTTP API endpoint."""
+
+    def __init__(self, url: str, api_key: str):
+        self.url = url
+        self.api_key = api_key
+        self.client = httpx.AsyncClient()
+
+    async def write(self, event_dict):
+        """Write log event to HTTP API."""
+        headers = {"Authorization": f"Bearer {self.api_key}"}
+        await self.client.post(self.url, json=event_dict, headers=headers)
+
+    async def close(self):
+        """Clean up resources."""
+        await self.client.aclose()
+
+class FileSink(Sink):
+    """Write logs to rotating files."""
+
+    def __init__(self, filename: str):
+        self.filename = filename
+
+    async def write(self, event_dict):
+        """Write log event to file."""
+        import json
+        with open(self.filename, "a") as f:
+            f.write(json.dumps(event_dict) + "\n")
+
+    async def close(self):
+        """Clean up resources."""
+        pass
+```
+
+**Sink Interface:**
+
+```python
+from fapilog._internal.queue import Sink
+
+class CustomSink(Sink):
+    async def write(self, event_dict):
+        """
+        Write a log event.
+
+        Args:
+            event_dict: The structured log event dictionary
+        """
+        # Your sink implementation
+        pass
+
+    async def close(self):
+        """Clean up resources when sink is closed."""
+        pass
+```
+
+---
+
+## Context Management
+
+Context management allows you to add request-scoped metadata to all log events.
+
+### `bind_context()`
+
+Bind context variables that will be included in all subsequent log events.
+
+```python
+from fapilog._internal.context import bind_context
+
+# Bind single variable
+bind_context(user_id="123")
+
+# Bind multiple variables
+bind_context(
+    user_id="123",
+    session_id="abc456",
+    request_id="req789",
+    tenant="acme-corp"
+)
+
+# All subsequent logs will include these fields
+log.info("User action")  # Includes user_id, session_id, etc.
+```
+
+### `get_context()`
+
+Get the current context variables.
+
+```python
+from fapilog._internal.context import get_context
+
+context = get_context()
+print(context)  # {'user_id': '123', 'session_id': 'abc456', ...}
+```
+
+### `clear_context()`
+
+Clear all context variables.
+
+```python
+from fapilog._internal.context import clear_context
+
+# Clear context (useful for testing or request cleanup)
+clear_context()
+```
+
+**Context Variables:**
+
+Common context variables set by middleware:
+
+- `trace_id`: Request trace identifier
+- `span_id`: Request span identifier
+- `req_bytes`: Request body size
+- `res_bytes`: Response body size
+- `status_code`: HTTP status code
+- `latency_ms`: Request latency
+- `user_agent`: User-Agent header
+
+**Best Practices:**
+
+- Use context for request-scoped data
+- Clear context between requests in long-running processes
+- Avoid storing large objects in context
+- Use descriptive variable names
+
+---
+
+## Types and Models
+
+### `Sink`
+
+Base class for all log sinks.
+
+```python
+from fapilog._internal.queue import Sink
+
+class MySink(Sink):
+    async def write(self, event_dict):
+        # Implementation
+        pass
+
+    async def close(self):
+        # Cleanup
+        pass
+```
+
+### `QueueWorker`
+
+Manages the async log queue and sink processing.
+
+```python
+from fapilog._internal.queue import QueueWorker
+
+# Usually managed internally, but can be customized
+worker = QueueWorker(
+    sinks=[my_sink],
+    queue_size=1000,
+    batch_size=10,
+    batch_timeout=1.0
+)
+```
+
+### Environment Variables
+
+All configuration can be set via environment variables:
+
+| Variable                          | Default  | Description                    |
+| --------------------------------- | -------- | ------------------------------ |
+| `FAPILOG_LEVEL`                   | `INFO`   | Logging level                  |
+| `FAPILOG_SINKS`                   | `stdout` | Comma-separated sink list      |
+| `FAPILOG_JSON_CONSOLE`            | `auto`   | Console format                 |
+| `FAPILOG_QUEUE_ENABLED`           | `true`   | Enable async queue             |
+| `FAPILOG_QUEUE_MAXSIZE`           | `1000`   | Queue maximum size             |
+| `FAPILOG_QUEUE_OVERFLOW`          | `drop`   | Queue overflow strategy        |
+| `FAPILOG_QUEUE_BATCH_SIZE`        | `10`     | Events per batch               |
+| `FAPILOG_QUEUE_BATCH_TIMEOUT`     | `1.0`    | Batch timeout (seconds)        |
+| `FAPILOG_QUEUE_RETRY_DELAY`       | `1.0`    | Retry delay (seconds)          |
+| `FAPILOG_QUEUE_MAX_RETRIES`       | `3`      | Maximum retries                |
+| `FAPILOG_SAMPLING_RATE`           | `1.0`    | Log sampling rate              |
+| `FAPILOG_ENABLE_RESOURCE_METRICS` | `false`  | Enable resource metrics        |
+| `FAPILOG_REDACT_PATTERNS`         | ``       | Comma-separated regex patterns |
+
+---
+
+## Error Handling
+
+### Common Exceptions
+
+**`RuntimeError`**
+
+- Raised when `configure_logging()` is called from async context without proper setup
+- Solution: Ensure proper async context or use sync configuration
+
+**`ValueError`**
+
+- Raised for invalid configuration values
+- Common causes: Invalid log level, invalid queue settings
+- Solution: Check configuration values and environment variables
+
+**`ImportError`**
+
+- Raised when optional dependencies are missing
+- Common with `psutil` for resource metrics
+- Solution: Install required dependencies or disable features
+
+### Debugging Configuration
+
+```python
+import logging
+
+# Enable debug logging for fapilog itself
+logging.getLogger("fapilog").setLevel(logging.DEBUG)
+
+# Check configuration
+from fapilog.settings import LoggingSettings
+settings = LoggingSettings()
+print(settings.model_dump())
+```
+
+---
+
+## Performance Considerations
+
+### Queue Configuration
+
+For high-throughput applications:
+
+```python
+settings = LoggingSettings(
+    queue_maxsize=5000,        # Larger queue
+    queue_batch_size=50,       # Larger batches
+    queue_batch_timeout=0.5,   # Shorter timeout
+    queue_overflow="drop"      # Drop logs under load
+)
+```
+
+### Sampling
+
+For very high-volume logging:
+
+```python
+settings = LoggingSettings(
+    sampling_rate=0.1,  # Log only 10% of events
+    queue_overflow="sample"  # Adaptive sampling under load
+)
+```
+
+### Resource Metrics
+
+Enable only when needed:
+
+```python
+settings = LoggingSettings(
+    enable_resource_metrics=False  # Disable for performance
+)
+```
+
+---
+
+## Migration Guide
+
+### From Standard Logging
+
+```python
+# Before
+import logging
+logging.basicConfig(level=logging.INFO)
+logger = logging.getLogger(__name__)
+logger.info("User logged in")
+
+# After
+from fapilog import configure_logging, log
+configure_logging()
+log.info("User logged in")
+```
+
+### From Structlog
+
+```python
+# Before
+import structlog
+structlog.configure(processors=[...])
+logger = structlog.get_logger()
+logger.info("User logged in")
+
+# After
+from fapilog import configure_logging, log
+configure_logging()
+log.info("User logged in")
+```
+
+### From Custom Middleware
+
+```python
+# Before
+@app.middleware("http")
+async def add_trace_id(request, call_next):
+    # Custom trace ID logic
+    pass
+
+# After
+from fapilog import configure_logging
+configure_logging(app=app)  # Automatic trace ID handling
+```
diff --git a/docs/documentation-structure.md b/docs/documentation-structure.md
new file mode 100644
index 0000000..840d525
--- /dev/null
+++ b/docs/documentation-structure.md
@@ -0,0 +1,313 @@
+# Documentation Structure
+
+This document outlines the complete documentation structure for `fapilog`, showing how all documentation pieces work together to provide a comprehensive developer experience.
+
+---
+
+## üìö Documentation Overview
+
+The `fapilog` documentation is organized into several interconnected sections, each serving a specific purpose in the developer journey:
+
+```
+docs/
+‚îú‚îÄ‚îÄ README.md                    # Project overview and quick start
+‚îú‚îÄ‚îÄ api-reference.md            # Complete API documentation
+‚îú‚îÄ‚îÄ user-guide.md              # Step-by-step tutorials
+‚îú‚îÄ‚îÄ deployment-guide.md        # Production deployment
+‚îú‚îÄ‚îÄ architecture.md            # System design and internals
+‚îú‚îÄ‚îÄ contributing.md            # Development guidelines
+‚îú‚îÄ‚îÄ security.md               # Security considerations
+‚îú‚îÄ‚îÄ performance.md            # Performance tuning
+‚îú‚îÄ‚îÄ migration-guide.md        # Migration from other logging
+‚îú‚îÄ‚îÄ troubleshooting.md        # Common issues and solutions
+‚îî‚îÄ‚îÄ examples/                 # Code examples
+    ‚îú‚îÄ‚îÄ basic-setup.md
+    ‚îú‚îÄ‚îÄ advanced-configuration.md
+    ‚îú‚îÄ‚îÄ custom-enrichers.md
+    ‚îú‚îÄ‚îÄ custom-sinks.md
+    ‚îî‚îÄ‚îÄ integration-guides.md
+```
+
+---
+
+## üéØ Documentation Sections
+
+### 1. **API Reference** (`api-reference.md`)
+
+**Purpose:** Complete technical reference for all public APIs
+**Audience:** Developers who need detailed API information
+**Content:**
+
+- Complete function signatures and parameters
+- Type definitions and models
+- Code examples for every API
+- Error handling and exceptions
+- Performance considerations
+- Migration guides
+
+**Key Features:**
+
+- Comprehensive coverage of all public APIs
+- Clear parameter descriptions with types
+- Practical code examples
+- Error handling guidance
+- Performance optimization tips
+
+### 2. **User Guide** (`user-guide.md`)
+
+**Purpose:** Progressive tutorials from basic to advanced usage
+**Audience:** Developers learning to use `fapilog`
+**Content:**
+
+- Getting Started (5-minute setup)
+- Basic Configuration
+- Advanced Configuration
+- Custom Enrichers
+- Custom Sinks
+- Performance Tuning
+- Troubleshooting
+
+**Structure:**
+
+```
+1. Quick Start
+   - Installation
+   - Basic Setup
+   - First Log
+
+2. Basic Usage
+   - Configuration Options
+   - Logging Levels
+   - Structured Logging
+
+3. Advanced Features
+   - Custom Enrichers
+   - Custom Sinks
+   - Context Management
+
+4. Production Setup
+   - Environment Configuration
+   - Performance Tuning
+   - Monitoring Integration
+```
+
+### 3. **Deployment Guide** (`deployment-guide.md`)
+
+**Purpose:** Production-ready deployment instructions
+**Audience:** DevOps engineers and platform teams
+**Content:**
+
+- Docker Deployment
+- Kubernetes Deployment
+- Environment Configuration
+- Monitoring & Alerting
+- Log Aggregation Setup
+- Performance Tuning
+
+### 4. **Architecture Documentation** (`architecture.md`)
+
+**Purpose:** Technical deep-dive into system design
+**Audience:** Contributors and advanced users
+**Content:**
+
+- System Architecture Diagram
+- Component Interactions
+- Data Flow Diagrams
+- Performance Characteristics
+- Design Decisions
+
+### 5. **Contributing Guidelines** (`contributing.md`)
+
+**Purpose:** Guide for contributors and maintainers
+**Audience:** Potential contributors
+**Content:**
+
+- Development Setup
+- Code Style Guide
+- Testing Guidelines
+- Pull Request Process
+- Release Process
+- Architecture Decision Records (ADRs)
+
+### 6. **Security Documentation** (`security.md`)
+
+**Purpose:** Security best practices and considerations
+**Audience:** Security-conscious teams
+**Content:**
+
+- PII Handling
+- Log Sanitization
+- Security Configuration
+- Compliance (GDPR, SOC2)
+- Vulnerability Reporting
+
+### 7. **Performance Guide** (`performance.md`)
+
+**Purpose:** Performance analysis and optimization
+**Audience:** Performance-conscious developers
+**Content:**
+
+- Performance Benchmarks
+- Load Testing Results
+- Memory Usage Analysis
+- CPU Impact Analysis
+- Optimization Guidelines
+
+### 8. **Migration Guide** (`migration-guide.md`)
+
+**Purpose:** Help users migrate from other logging solutions
+**Audience:** Teams with existing logging
+**Content:**
+
+- From `logging` module
+- From `structlog`
+- From other logging libraries
+- Step-by-step migration checklist
+
+### 9. **Troubleshooting** (`troubleshooting.md`)
+
+**Purpose:** Common issues and solutions
+**Audience:** All users
+**Content:**
+
+- Common Problems
+- Debugging Guide
+- Performance Issues
+- Configuration Issues
+- Error Messages
+
+---
+
+## üîó Documentation Relationships
+
+### **Entry Points**
+
+1. **README.md** - Main entry point with quick start
+2. **User Guide** - Progressive learning path
+3. **API Reference** - Complete technical reference
+
+### **Cross-References**
+
+- User Guide references specific API sections
+- API Reference links to relevant examples
+- Deployment Guide references architecture
+- Contributing Guide references development setup
+
+### **Audience Flow**
+
+```
+New User ‚Üí README ‚Üí User Guide ‚Üí API Reference
+Experienced User ‚Üí API Reference ‚Üí Performance Guide
+DevOps ‚Üí Deployment Guide ‚Üí Architecture
+Contributor ‚Üí Contributing Guide ‚Üí Architecture
+```
+
+---
+
+## üìñ Documentation Standards
+
+### **Writing Style**
+
+- Clear, concise language
+- Practical examples for every concept
+- Progressive complexity (simple to advanced)
+- Consistent terminology
+
+### **Code Examples**
+
+- Complete, runnable examples
+- Multiple complexity levels
+- Real-world scenarios
+- Error handling included
+
+### **Structure**
+
+- Consistent heading hierarchy
+- Table of contents for each document
+- Cross-references between documents
+- Search-friendly content
+
+### **Maintenance**
+
+- Version-specific documentation
+- Changelog integration
+- Automated testing of examples
+- Regular review and updates
+
+---
+
+## üõ† Documentation Tools
+
+### **Recommended Stack**
+
+- **Markdown** - Primary format
+- **MkDocs** - Documentation site generator
+- **Material for MkDocs** - Modern theme
+- **mkdocstrings** - Auto-generated API docs
+- **GitHub Pages** - Hosting
+
+### **Automation**
+
+- Auto-generated API reference from docstrings
+- Example code testing
+- Link validation
+- Spell checking
+- Format validation
+
+### **Integration**
+
+- GitHub Actions for automated builds
+- Read the Docs integration
+- Search functionality
+- Version-specific documentation
+
+---
+
+## üìã Implementation Priority
+
+### **Phase 1: Core Documentation** (High Priority)
+
+1. ‚úÖ API Reference (Complete)
+2. User Guide (Progressive tutorials)
+3. Contributing Guidelines (Community growth)
+4. Migration Guide (Adoption)
+
+### **Phase 2: Production Documentation** (Medium Priority)
+
+5. Deployment Guide (Production readiness)
+6. Architecture Documentation (Technical understanding)
+7. Security Documentation (Enterprise adoption)
+8. Troubleshooting (User support)
+
+### **Phase 3: Advanced Documentation** (Lower Priority)
+
+9. Performance Guide (Advanced users)
+10. Integration Guides (Ecosystem integration)
+11. Community Documentation (Long-term sustainability)
+
+---
+
+## üéØ Success Metrics
+
+### **Documentation Quality**
+
+- 90%+ API coverage
+- All examples tested and working
+- Zero broken links
+- Consistent terminology
+
+### **Developer Experience**
+
+- 5-minute setup time
+- Clear migration path
+- Comprehensive troubleshooting
+- Searchable content
+
+### **Community Engagement**
+
+- Clear contribution guidelines
+- Active documentation updates
+- Community feedback integration
+- Regular content reviews
+
+This documentation structure ensures that `fapilog` provides a professional, comprehensive developer experience that supports users at every stage of their journey.
diff --git a/docs/user-guide.md b/docs/user-guide.md
new file mode 100644
index 0000000..c506394
--- /dev/null
+++ b/docs/user-guide.md
@@ -0,0 +1,1076 @@
+# User Guide
+
+This guide provides step-by-step tutorials for using `fapilog` in your applications. It's designed to take you from basic setup to advanced features, with practical examples throughout.
+
+---
+
+## Table of Contents
+
+- [Quick Start](#quick-start)
+- [Basic Configuration](#basic-configuration)
+- [FastAPI Integration](#fastapi-integration)
+- [Advanced Configuration](#advanced-configuration)
+- [Custom Enrichers](#custom-enrichers)
+- [Custom Sinks](#custom-sinks)
+- [Performance Tuning](#performance-tuning)
+- [Production Deployment](#production-deployment)
+- [Troubleshooting](#troubleshooting)
+
+---
+
+## Quick Start
+
+Get up and running with `fapilog` in under 5 minutes.
+
+### Installation
+
+```bash
+pip install fapilog
+```
+
+### Basic Setup
+
+The simplest way to get started:
+
+```python
+from fapilog import configure_logging, log
+
+# Configure logging with defaults
+configure_logging()
+
+# Start logging
+log.info("Application started", version="1.0.0")
+log.warning("Deprecated feature used", feature="old_api")
+log.error("Database connection failed", database="postgres")
+```
+
+**What happens:**
+
+- ‚úÖ Structured JSON logging enabled
+- ‚úÖ Automatic timestamp and log level
+- ‚úÖ Hostname and process ID added
+- ‚úÖ Pretty console output in development
+- ‚úÖ Async-safe logging with background queue
+
+### Your First Log
+
+```python
+from fapilog import configure_logging, log
+
+configure_logging()
+
+# Basic logging
+log.info("User logged in", user_id="123", action="login")
+
+# Structured logging with multiple fields
+log.info(
+    "API request processed",
+    endpoint="/api/users",
+    method="GET",
+    status_code=200,
+    duration_ms=45.2,
+    user_id="123"
+)
+```
+
+**Output:**
+
+```json
+{
+  "timestamp": "2024-01-15T10:30:45.123Z",
+  "level": "info",
+  "event": "User logged in",
+  "hostname": "server-01",
+  "pid": 12345,
+  "user_id": "123",
+  "action": "login"
+}
+```
+
+---
+
+## Basic Configuration
+
+Learn how to configure `fapilog` for different environments and use cases.
+
+### Environment-Based Configuration
+
+`fapilog` uses environment variables for easy configuration:
+
+```bash
+# Development
+export FAPILOG_LEVEL=DEBUG
+export FAPILOG_JSON_CONSOLE=pretty
+export FAPILOG_QUEUE_ENABLED=false
+
+# Production
+export FAPILOG_LEVEL=INFO
+export FAPILOG_JSON_CONSOLE=json
+export FAPILOG_QUEUE_ENABLED=true
+export FAPILOG_ENABLE_RESOURCE_METRICS=true
+```
+
+### Programmatic Configuration
+
+Use `LoggingSettings` for programmatic control:
+
+```python
+from fapilog import configure_logging
+from fapilog.settings import LoggingSettings
+
+# Development settings
+dev_settings = LoggingSettings(
+    level="DEBUG",
+    json_console="pretty",
+    queue_enabled=False,
+    enable_resource_metrics=False
+)
+
+# Production settings
+prod_settings = LoggingSettings(
+    level="INFO",
+    json_console="json",
+    queue_enabled=True,
+    queue_maxsize=1000,
+    enable_resource_metrics=True
+)
+
+configure_logging(settings=dev_settings)
+```
+
+### Configuration Options
+
+| Setting          | Environment Variable              | Default | Description                        |
+| ---------------- | --------------------------------- | ------- | ---------------------------------- |
+| Log Level        | `FAPILOG_LEVEL`                   | `INFO`  | DEBUG, INFO, WARN, ERROR, CRITICAL |
+| Console Format   | `FAPILOG_JSON_CONSOLE`            | `auto`  | auto, json, pretty                 |
+| Queue Enabled    | `FAPILOG_QUEUE_ENABLED`           | `true`  | Enable async queue                 |
+| Queue Size       | `FAPILOG_QUEUE_MAXSIZE`           | `1000`  | Maximum queue size                 |
+| Resource Metrics | `FAPILOG_ENABLE_RESOURCE_METRICS` | `false` | Memory/CPU metrics                 |
+| Sampling Rate    | `FAPILOG_SAMPLING_RATE`           | `1.0`   | Log sampling (0.0-1.0)             |
+
+### Development vs Production
+
+**Development Configuration:**
+
+```python
+settings = LoggingSettings(
+    level="DEBUG",
+    json_console="pretty",  # Human-readable output
+    queue_enabled=False,    # Simpler setup
+    enable_resource_metrics=False  # Better performance
+)
+```
+
+**Production Configuration:**
+
+```python
+settings = LoggingSettings(
+    level="INFO",
+    json_console="json",    # Structured for log aggregation
+    queue_enabled=True,     # Non-blocking logging
+    queue_maxsize=2000,     # Larger queue for high load
+    enable_resource_metrics=True,  # Monitor resources
+    sampling_rate=1.0       # Log everything
+)
+```
+
+---
+
+## FastAPI Integration
+
+Integrate `fapilog` with FastAPI applications for automatic request correlation and structured logging.
+
+### Basic FastAPI Setup
+
+```python
+from fastapi import FastAPI
+from fapilog import configure_logging, log
+
+# Configure logging with FastAPI integration
+configure_logging()
+
+# Create FastAPI app
+app = FastAPI(title="My API")
+
+# Log application startup
+log.info("FastAPI application starting", app_name="my-api", version="1.0.0")
+
+@app.get("/")
+async def root():
+    log.info("Root endpoint accessed")
+    return {"message": "Hello World"}
+
+@app.get("/users/{user_id}")
+async def get_user(user_id: int):
+    log.info("User requested", user_id=user_id, endpoint="/users/{user_id}")
+
+    # Your business logic here
+    user = {"id": user_id, "name": "John Doe"}
+
+    log.info("User found", user_id=user_id, user_name=user["name"])
+    return user
+```
+
+**What you get automatically:**
+
+- ‚úÖ Trace ID generation and correlation
+- ‚úÖ Request timing and latency
+- ‚úÖ HTTP status codes and response sizes
+- ‚úÖ Request path and method logging
+- ‚úÖ Error handling with structured logs
+
+### Advanced FastAPI Patterns
+
+**Request/Response Logging:**
+
+```python
+from fastapi import Request, Response
+from fapilog import log
+
+@app.middleware("http")
+async def log_requests(request: Request, call_next):
+    # Log request start
+    log.info(
+        "Request started",
+        path=request.url.path,
+        method=request.method,
+        client_ip=request.client.host
+    )
+
+    # Process request
+    response = await call_next(request)
+
+    # Log response
+    log.info(
+        "Request completed",
+        path=request.url.path,
+        method=request.method,
+        status_code=response.status_code,
+        duration_ms=calculate_duration()
+    )
+
+    return response
+```
+
+**Error Handling:**
+
+```python
+from fastapi import HTTPException
+from fapilog import log
+
+@app.exception_handler(HTTPException)
+async def http_exception_handler(request: Request, exc: HTTPException):
+    log.error(
+        "HTTP exception occurred",
+        path=request.url.path,
+        method=request.method,
+        status_code=exc.status_code,
+        detail=exc.detail
+    )
+    return {"error": exc.detail}
+
+@app.exception_handler(Exception)
+async def global_exception_handler(request: Request, exc: Exception):
+    log.error(
+        "Unhandled exception",
+        path=request.url.path,
+        method=request.method,
+        error=str(exc),
+        error_type=type(exc).__name__
+    )
+    return {"error": "Internal server error"}
+```
+
+**Business Metrics:**
+
+```python
+@app.post("/orders")
+async def create_order(order: Order):
+    log.info(
+        "Order creation started",
+        customer_id=order.customer_id,
+        item_count=len(order.items),
+        total_amount=calculate_total(order.items)
+    )
+
+    # Process order...
+
+    log.info(
+        "Order created successfully",
+        order_id=order_id,
+        customer_id=order.customer_id,
+        total_amount=total_amount
+    )
+
+    return {"order_id": order_id}
+```
+
+### Trace Correlation
+
+`fapilog` automatically handles trace correlation:
+
+**Request Headers:**
+
+- `X-Trace-Id`: Request trace identifier
+- `X-Span-Id`: Request span identifier
+- `X-Response-Time-ms`: Request latency
+
+**Response Headers:**
+
+```python
+# Automatically added to responses
+response.headers["X-Trace-Id"] = trace_id
+response.headers["X-Span-Id"] = span_id
+response.headers["X-Response-Time-ms"] = "45.2"
+```
+
+---
+
+## Advanced Configuration
+
+Configure `fapilog` for high-performance and production environments.
+
+### Queue Configuration
+
+**High-Throughput Applications:**
+
+```python
+settings = LoggingSettings(
+    queue_maxsize=5000,        # Larger queue
+    queue_batch_size=50,       # Larger batches
+    queue_batch_timeout=0.5,   # Shorter timeout
+    queue_overflow="drop"      # Drop logs under load
+)
+```
+
+**Guaranteed Log Delivery:**
+
+```python
+settings = LoggingSettings(
+    queue_overflow="block",    # Wait for space
+    queue_maxsize=1000,
+    queue_retry_delay=2.0,    # Longer retry delay
+    queue_max_retries=5       # More retries
+)
+```
+
+**Adaptive Sampling:**
+
+```python
+settings = LoggingSettings(
+    queue_overflow="sample",   # Probabilistic sampling
+    sampling_rate=0.1,        # Keep 10% of logs
+    queue_maxsize=1000
+)
+```
+
+### Performance Tuning
+
+**Memory Optimization:**
+
+```python
+settings = LoggingSettings(
+    enable_resource_metrics=False,  # Disable for performance
+    queue_batch_size=100,          # Larger batches
+    queue_batch_timeout=2.0        # Longer timeouts
+)
+```
+
+**CPU Optimization:**
+
+```python
+settings = LoggingSettings(
+    queue_enabled=True,           # Async processing
+    queue_overflow="drop",        # Don't block
+    sampling_rate=0.5            # Sample under load
+)
+```
+
+### Security Configuration
+
+**PII Redaction:**
+
+```python
+settings = LoggingSettings(
+    redact_patterns=[
+        r"password=\w+",
+        r"token=\w+",
+        r"ssn=\d{3}-\d{2}-\d{4}"
+    ]
+)
+```
+
+**Field Filtering:**
+
+```python
+# In your enrichers
+def security_enricher(logger, method_name, event_dict):
+    # Remove sensitive fields
+    event_dict.pop("password", None)
+    event_dict.pop("api_key", None)
+    return event_dict
+```
+
+---
+
+## Custom Enrichers
+
+Add application-specific metadata to all log events automatically.
+
+### Basic Custom Enricher
+
+```python
+from fapilog.enrichers import register_enricher
+from fapilog import configure_logging, log
+
+def user_context_enricher(logger, method_name, event_dict):
+    """Add user context from session."""
+    # Get user from your session/context
+    user = get_current_user()
+    if user:
+        event_dict["user_id"] = user.id
+        event_dict["user_role"] = user.role
+    return event_dict
+
+def environment_enricher(logger, method_name, event_dict):
+    """Add environment information."""
+    import os
+    event_dict["environment"] = os.getenv("ENVIRONMENT", "development")
+    event_dict["service_name"] = "my-fastapi-app"
+    return event_dict
+
+# Register enrichers
+register_enricher(user_context_enricher)
+register_enricher(environment_enricher)
+
+# Configure logging
+configure_logging()
+
+# All logs now include user and environment info
+log.info("API request processed", endpoint="/api/users")
+```
+
+### Advanced Enrichers
+
+**Request-Scoped Enricher:**
+
+```python
+from fapilog._internal.context import get_context
+
+def request_enricher(logger, method_name, event_dict):
+    """Add request-scoped data."""
+    context = get_context()
+
+    # Add request metadata
+    if "trace_id" in context:
+        event_dict["trace_id"] = context["trace_id"]
+
+    # Add business context
+    if "tenant_id" in context:
+        event_dict["tenant_id"] = context["tenant_id"]
+
+    return event_dict
+
+register_enricher(request_enricher)
+```
+
+**Performance Enricher:**
+
+```python
+import time
+import psutil
+
+def performance_enricher(logger, method_name, event_dict):
+    """Add performance metrics."""
+    process = psutil.Process()
+
+    # Memory usage
+    memory_info = process.memory_info()
+    event_dict["memory_mb"] = round(memory_info.rss / (1024 * 1024), 2)
+
+    # CPU usage
+    event_dict["cpu_percent"] = round(process.cpu_percent(), 2)
+
+    return event_dict
+
+register_enricher(performance_enricher)
+```
+
+**Business Metrics Enricher:**
+
+```python
+def business_metrics_enricher(logger, method_name, event_dict):
+    """Add business-specific metrics."""
+    # Add business context
+    event_dict["business_unit"] = "e-commerce"
+    event_dict["feature_flag"] = get_feature_flag("new_ui")
+
+    # Add timing if available
+    if "duration_ms" in event_dict:
+        if event_dict["duration_ms"] > 1000:
+            event_dict["performance_category"] = "slow"
+        elif event_dict["duration_ms"] > 100:
+            event_dict["performance_category"] = "medium"
+        else:
+            event_dict["performance_category"] = "fast"
+
+    return event_dict
+
+register_enricher(business_metrics_enricher)
+```
+
+### Enricher Management
+
+**Register Multiple Enrichers:**
+
+```python
+from fapilog.enrichers import register_enricher, clear_enrichers
+
+# Clear existing enrichers
+clear_enrichers()
+
+# Register new enrichers
+register_enricher(user_enricher)
+register_enricher(performance_enricher)
+register_enricher(business_enricher)
+```
+
+**Conditional Enrichers:**
+
+```python
+def conditional_enricher(logger, method_name, event_dict):
+    """Only add fields in certain conditions."""
+    if method_name == "error":
+        event_dict["error_timestamp"] = time.time()
+        event_dict["error_count"] = get_error_count()
+
+    return event_dict
+```
+
+---
+
+## Custom Sinks
+
+Create custom sinks for specialized logging requirements.
+
+### Basic Custom Sink
+
+```python
+from fapilog._internal.queue import Sink
+import json
+
+class FileSink(Sink):
+    """Write logs to a file."""
+
+    def __init__(self, filename: str):
+        self.filename = filename
+
+    async def write(self, event_dict):
+        """Write log event to file."""
+        with open(self.filename, "a") as f:
+            f.write(json.dumps(event_dict) + "\n")
+
+    async def close(self):
+        """Clean up resources."""
+        pass
+```
+
+### HTTP API Sink
+
+```python
+import aiohttp
+from fapilog._internal.queue import Sink
+
+class HTTPApiSink(Sink):
+    """Send logs to HTTP API endpoint."""
+
+    def __init__(self, url: str, api_key: str):
+        self.url = url
+        self.api_key = api_key
+        self.session = None
+
+    async def start(self):
+        """Initialize the sink."""
+        self.session = aiohttp.ClientSession()
+
+    async def write(self, event_dict):
+        """Send log to HTTP API."""
+        headers = {"Authorization": f"Bearer {self.api_key}"}
+        await self.session.post(self.url, json=event_dict, headers=headers)
+
+    async def close(self):
+        """Clean up resources."""
+        if self.session:
+            await self.session.close()
+```
+
+### Audit Log Sink
+
+```python
+import json
+import time
+from fapilog._internal.queue import Sink
+
+class AuditLogSink(Sink):
+    """Specialized sink for audit logging."""
+
+    def __init__(self, audit_file: str):
+        self.audit_file = audit_file
+
+    async def write(self, event_dict):
+        """Write audit log with special formatting."""
+        audit_entry = {
+            "timestamp": time.time(),
+            "audit_type": "user_action",
+            "user_id": event_dict.get("user_id"),
+            "action": event_dict.get("action"),
+            "resource": event_dict.get("resource"),
+            "ip_address": event_dict.get("ip_address"),
+            "user_agent": event_dict.get("user_agent")
+        }
+
+        with open(self.audit_file, "a") as f:
+            f.write(json.dumps(audit_entry) + "\n")
+```
+
+### Using Custom Sinks
+
+**Register with Queue Worker:**
+
+```python
+from fapilog._internal.queue import QueueWorker
+
+# Create custom sinks
+file_sink = FileSink("app.log")
+api_sink = HTTPApiSink("https://logs.example.com/api", "api-key")
+
+# Create queue worker with custom sinks
+worker = QueueWorker(
+    sinks=[file_sink, api_sink],
+    queue_size=1000,
+    batch_size=10
+)
+```
+
+**Integration with FastAPI:**
+
+```python
+from fastapi import FastAPI
+from fapilog import configure_logging
+
+app = FastAPI()
+
+# Configure with custom sinks
+settings = LoggingSettings(
+    queue_enabled=True,
+    queue_maxsize=1000
+)
+
+configure_logging(settings=settings)
+
+# Your custom sinks will be used automatically
+```
+
+---
+
+## Performance Tuning
+
+Optimize `fapilog` for your specific performance requirements.
+
+### High-Throughput Scenarios
+
+**API with 10K+ requests/second:**
+
+```python
+settings = LoggingSettings(
+    queue_maxsize=10000,       # Large queue
+    queue_batch_size=100,      # Large batches
+    queue_batch_timeout=0.1,   # Short timeout
+    queue_overflow="drop",     # Drop under load
+    sampling_rate=0.1          # Sample 10% of logs
+)
+```
+
+**Memory-Constrained Environments:**
+
+```python
+settings = LoggingSettings(
+    queue_maxsize=100,         # Small queue
+    queue_batch_size=5,        # Small batches
+    enable_resource_metrics=False,  # Disable metrics
+    queue_overflow="block"     # Wait for space
+)
+```
+
+### Load Testing
+
+**Test Queue Performance:**
+
+```python
+import asyncio
+from fapilog import configure_logging, log
+
+# Configure for load testing
+settings = LoggingSettings(
+    queue_maxsize=1000,
+    queue_batch_size=50,
+    queue_overflow="drop"
+)
+
+configure_logging(settings=settings)
+
+async def load_test():
+    """Generate high-volume logs."""
+    for i in range(10000):
+        log.info(f"load_test_message_{i}", iteration=i)
+        await asyncio.sleep(0.001)  # 1ms between logs
+
+# Run load test
+asyncio.run(load_test())
+```
+
+### Monitoring Performance
+
+**Enable Resource Metrics:**
+
+```python
+settings = LoggingSettings(
+    enable_resource_metrics=True,
+    level="INFO"
+)
+
+configure_logging(settings=settings)
+
+# Log events will include memory and CPU usage
+log.info("Performance check", operation="database_query")
+```
+
+**Monitor Queue Health:**
+
+```python
+from fapilog._internal.queue import get_queue_stats
+
+# Get queue statistics
+stats = get_queue_stats()
+log.info(
+    "Queue health check",
+    queue_size=stats.queue_size,
+    queue_maxsize=stats.queue_maxsize,
+    dropped_events=stats.dropped_events
+)
+```
+
+---
+
+## Production Deployment
+
+Deploy `fapilog` in production environments with proper configuration and monitoring.
+
+### Docker Deployment
+
+**Dockerfile:**
+
+```dockerfile
+FROM python:3.11-slim
+
+WORKDIR /app
+
+# Install dependencies
+COPY requirements.txt .
+RUN pip install -r requirements.txt
+
+# Copy application
+COPY . .
+
+# Set production environment
+ENV FAPILOG_LEVEL=INFO
+ENV FAPILOG_JSON_CONSOLE=json
+ENV FAPILOG_QUEUE_ENABLED=true
+ENV FAPILOG_ENABLE_RESOURCE_METRICS=true
+
+# Run application
+CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
+```
+
+**docker-compose.yml:**
+
+```yaml
+version: "3.8"
+services:
+  api:
+    build: .
+    ports:
+      - "8000:8000"
+    environment:
+      - FAPILOG_LEVEL=INFO
+      - FAPILOG_QUEUE_ENABLED=true
+      - FAPILOG_QUEUE_MAXSIZE=2000
+    volumes:
+      - ./logs:/app/logs
+```
+
+### Kubernetes Deployment
+
+**ConfigMap:**
+
+```yaml
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: fapilog-config
+data:
+  FAPILOG_LEVEL: "INFO"
+  FAPILOG_JSON_CONSOLE: "json"
+  FAPILOG_QUEUE_ENABLED: "true"
+  FAPILOG_QUEUE_MAXSIZE: "2000"
+  FAPILOG_ENABLE_RESOURCE_METRICS: "true"
+```
+
+**Deployment:**
+
+```yaml
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: my-api
+spec:
+  replicas: 3
+  selector:
+    matchLabels:
+      app: my-api
+  template:
+    metadata:
+      labels:
+        app: my-api
+    spec:
+      containers:
+        - name: api
+          image: my-api:latest
+          envFrom:
+            - configMapRef:
+                name: fapilog-config
+          ports:
+            - containerPort: 8000
+```
+
+### Environment Configuration
+
+**Development:**
+
+```bash
+export FAPILOG_LEVEL=DEBUG
+export FAPILOG_JSON_CONSOLE=pretty
+export FAPILOG_QUEUE_ENABLED=false
+export FAPILOG_ENABLE_RESOURCE_METRICS=false
+```
+
+**Staging:**
+
+```bash
+export FAPILOG_LEVEL=INFO
+export FAPILOG_JSON_CONSOLE=json
+export FAPILOG_QUEUE_ENABLED=true
+export FAPILOG_QUEUE_MAXSIZE=1000
+export FAPILOG_ENABLE_RESOURCE_METRICS=true
+```
+
+**Production:**
+
+```bash
+export FAPILOG_LEVEL=WARN
+export FAPILOG_JSON_CONSOLE=json
+export FAPILOG_QUEUE_ENABLED=true
+export FAPILOG_QUEUE_MAXSIZE=5000
+export FAPILOG_QUEUE_OVERFLOW=drop
+export FAPILOG_SAMPLING_RATE=0.1
+export FAPILOG_ENABLE_RESOURCE_METRICS=true
+```
+
+### Monitoring Integration
+
+**Prometheus Metrics:**
+
+```python
+from prometheus_client import Counter, Histogram
+from fapilog import log
+
+# Define metrics
+log_events_total = Counter('log_events_total', 'Total log events', ['level'])
+log_event_duration = Histogram('log_event_duration_seconds', 'Log event duration')
+
+def monitored_logger(level, message, **kwargs):
+    """Log with metrics."""
+    log_events_total.labels(level=level).inc()
+
+    with log_event_duration.time():
+        getattr(log, level)(message, **kwargs)
+```
+
+**Health Checks:**
+
+```python
+@app.get("/health/logging")
+async def logging_health():
+    """Check logging system health."""
+    try:
+        # Test logging
+        log.info("Health check", check_type="logging")
+
+        # Get queue stats
+        stats = get_queue_stats()
+
+        return {
+            "status": "healthy",
+            "queue_size": stats.queue_size,
+            "queue_maxsize": stats.queue_maxsize
+        }
+    except Exception as e:
+        log.error("Logging health check failed", error=str(e))
+        raise HTTPException(status_code=503, detail="Logging unhealthy")
+```
+
+---
+
+## Troubleshooting
+
+Common issues and their solutions.
+
+### Configuration Issues
+
+**Logs not appearing:**
+
+```python
+# Check configuration
+from fapilog.settings import LoggingSettings
+settings = LoggingSettings()
+print(settings.model_dump())
+
+# Enable debug logging
+import logging
+logging.getLogger("fapilog").setLevel(logging.DEBUG)
+```
+
+**Queue not working:**
+
+```python
+# Check queue configuration
+settings = LoggingSettings(queue_enabled=True)
+configure_logging(settings=settings)
+
+# Test queue
+log.info("Queue test")
+```
+
+### Performance Issues
+
+**High memory usage:**
+
+```python
+# Reduce queue size
+settings = LoggingSettings(
+    queue_maxsize=100,
+    queue_batch_size=5,
+    enable_resource_metrics=False
+)
+```
+
+**Slow logging:**
+
+```python
+# Enable async queue
+settings = LoggingSettings(
+    queue_enabled=True,
+    queue_overflow="drop",
+    queue_batch_size=50
+)
+```
+
+### Integration Issues
+
+**FastAPI middleware not working:**
+
+```python
+# Ensure middleware is registered
+from fapilog import configure_logging
+app = FastAPI()
+configure_logging(app=app)  # This registers middleware
+```
+
+**Custom enrichers not working:**
+
+```python
+# Clear and re-register enrichers
+from fapilog.enrichers import clear_enrichers, register_enricher
+
+clear_enrichers()
+register_enricher(my_enricher)
+configure_logging()
+```
+
+### Debug Mode
+
+**Enable debug logging:**
+
+```python
+import logging
+logging.getLogger("fapilog").setLevel(logging.DEBUG)
+
+# Configure with debug
+settings = LoggingSettings(level="DEBUG")
+configure_logging(settings=settings)
+```
+
+**Test configuration:**
+
+```python
+# Test all components
+log.debug("Debug test")
+log.info("Info test")
+log.warning("Warning test")
+log.error("Error test")
+```
+
+### Common Error Messages
+
+**"RuntimeError: Cannot configure logging from async context"**
+
+- Solution: Configure logging before entering async context
+
+**"ValueError: Invalid level 'INVALID'"**
+
+- Solution: Use valid log levels: DEBUG, INFO, WARN, ERROR, CRITICAL
+
+**"ImportError: No module named 'psutil'"**
+
+- Solution: Install psutil or disable resource metrics
+
+---
+
+## Next Steps
+
+Now that you've learned the basics, explore these advanced topics:
+
+1. **API Reference** - Complete technical documentation
+2. **Custom Sinks** - Create specialized log destinations
+3. **Performance Tuning** - Optimize for your use case
+4. **Production Deployment** - Deploy with confidence
+5. **Contributing** - Help improve `fapilog`
+
+### Examples to Try
+
+- [Basic Setup](examples/01_basic_setup.py) - Minimal configuration
+- [Environment Config](examples/02_environment_config.py) - Environment-based setup
+- [FastAPI Basic](examples/05_fastapi_basic.py) - FastAPI integration
+- [Structured Logging](examples/08_fastapi_structured_logging.py) - Advanced patterns
+- [Custom Enrichers](examples/custom_enricher_example.py) - Custom metadata
+- [Custom Sinks](examples/15_custom_sink.py) - Specialized logging
+
+### Getting Help
+
+- **Documentation**: [API Reference](api-reference.md)
+- **Examples**: Check the `examples/` directory
+- **Issues**: Report bugs on GitHub
+- **Discussions**: Ask questions in GitHub Discussions
+
+Happy logging! üöÄ
diff --git a/examples/01_basic_setup.py b/examples/01_basic_setup.py
new file mode 100644
index 0000000..d8a124f
--- /dev/null
+++ b/examples/01_basic_setup.py
@@ -0,0 +1,62 @@
+#!/usr/bin/env python3
+"""
+Example 1: Basic Setup
+
+This example demonstrates the minimal setup required to use fapilog.
+It shows how to configure logging and use the logger for basic operations.
+"""
+
+from fapilog import configure_logging, log
+
+
+def main():
+    """Demonstrate basic fapilog setup and usage."""
+    print("=== Basic Setup Example ===\n")
+
+    # Step 1: Configure logging with default settings
+    print("1. Configuring logging with default settings...")
+    configure_logging()
+    print("‚úÖ Logging configured successfully!\n")
+
+    # Step 2: Basic logging operations
+    print("2. Basic logging operations:")
+    print("-" * 40)
+
+    # Info level logging
+    log.info("Application started", version="1.0.0", environment="development")
+
+    # Warning level logging
+    log.warning("Deprecated feature used", feature="old_api", replacement="new_api")
+
+    # Error level logging
+    log.error(
+        "Failed to connect to database",
+        database="postgres",
+        error="connection timeout",
+        retry_count=3,
+    )
+
+    # Debug level logging (won't show with default INFO level)
+    log.debug("Debug information", debug_data="some debug info")
+
+    # Structured logging with multiple fields
+    log.info(
+        "User action completed",
+        user_id="user_123",
+        action="login",
+        ip_address="192.168.1.100",
+        user_agent="Mozilla/5.0...",
+        duration_ms=45.2,
+    )
+
+    print("\n3. Key features demonstrated:")
+    print("‚úÖ Structured JSON logging")
+    print("‚úÖ Multiple field support")
+    print("‚úÖ Log levels (INFO, WARNING, ERROR)")
+    print("‚úÖ Automatic timestamp and log level")
+    print("‚úÖ Hostname and process ID enrichment")
+    print("‚úÖ Trace ID generation (when used with FastAPI)")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/02_environment_config.py b/examples/02_environment_config.py
new file mode 100644
index 0000000..5de8984
--- /dev/null
+++ b/examples/02_environment_config.py
@@ -0,0 +1,127 @@
+#!/usr/bin/env python3
+"""
+Example 2: Environment Configuration
+
+This example demonstrates how to configure fapilog using environment variables
+and programmatic settings. It shows different configuration approaches.
+"""
+
+import os
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+def demonstrate_environment_config():
+    """Show how environment variables control logging behavior."""
+    print("=== Environment Configuration Example ===\n")
+
+    # Method 1: Using environment variables
+    print("1. Configuration via environment variables:")
+    print("-" * 50)
+
+    # Set environment variables for this example
+    os.environ["FAPILOG_LEVEL"] = "DEBUG"
+    os.environ["FAPILOG_JSON_CONSOLE"] = "json"
+    os.environ["FAPILOG_ENABLE_RESOURCE_METRICS"] = "true"
+
+    # Configure with environment variables
+    configure_logging()
+
+    print("Environment variables set:")
+    print("  FAPILOG_LEVEL=DEBUG")
+    print("  FAPILOG_JSON_CONSOLE=json")
+    print("  FAPILOG_ENABLE_RESOURCE_METRICS=true")
+    print()
+
+    # Log some events to see the configuration
+    log.debug("Debug message with environment config")
+    log.info("Info message with environment config")
+    log.warning("Warning message with environment config")
+
+    print("\n2. Configuration via LoggingSettings:")
+    print("-" * 50)
+
+    # Method 2: Using LoggingSettings programmatically
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",  # Pretty output for development
+        enable_resource_metrics=False,  # Disable resource metrics
+        queue_enabled=False,  # Disable queue for simpler example
+    )
+
+    # Configure with settings
+    configure_logging(settings=settings)
+
+    print("Programmatic settings:")
+    print("  level=INFO")
+    print("  json_console=pretty")
+    print("  enable_resource_metrics=False")
+    print("  queue_enabled=False")
+    print()
+
+    # Log some events to see the new configuration
+    log.info("Info message with programmatic config")
+    log.warning("Warning message with programmatic config")
+    log.error("Error message with programmatic config")
+
+    print("\n3. Available environment variables:")
+    print("-" * 50)
+    print(
+        "FAPILOG_LEVEL              - Logging level (DEBUG, INFO, WARN, ERROR, CRITICAL)"
+    )
+    print("FAPILOG_JSON_CONSOLE       - Console format (auto, json, pretty)")
+    print("FAPILOG_ENABLE_RESOURCE_METRICS - Enable memory/CPU metrics (true/false)")
+    print("FAPILOG_QUEUE_ENABLED      - Enable async queue (true/false)")
+    print("FAPILOG_QUEUE_MAXSIZE      - Maximum queue size (default: 1000)")
+    print("FAPILOG_QUEUE_OVERFLOW     - Overflow strategy (drop, block, sample)")
+    print("FAPILOG_SAMPLING_RATE      - Sampling rate (0.0 to 1.0)")
+    print("FAPILOG_REDACT_PATTERNS    - Comma-separated regex patterns to redact")
+    print("FAPILOG_SINKS              - Comma-separated list of sinks (stdout, loki)")
+
+
+def demonstrate_development_vs_production():
+    """Show different configurations for development vs production."""
+    print("\n4. Development vs Production Configuration:")
+    print("-" * 50)
+
+    # Development configuration
+    print("Development Configuration:")
+    dev_settings = LoggingSettings(
+        level="DEBUG",
+        json_console="pretty",  # Pretty output for readability
+        enable_resource_metrics=False,  # Disable for performance
+        queue_enabled=False,  # Disable queue for simplicity
+    )
+    configure_logging(settings=dev_settings)
+    log.info("Development environment log", env="development", debug_enabled=True)
+
+    # Production configuration
+    print("\nProduction Configuration:")
+    prod_settings = LoggingSettings(
+        level="INFO",
+        json_console="json",  # JSON for log aggregation
+        enable_resource_metrics=True,  # Enable for monitoring
+        queue_enabled=True,  # Enable queue for performance
+        queue_maxsize=1000,
+        queue_overflow="drop",
+    )
+    configure_logging(settings=prod_settings)
+    log.info("Production environment log", env="production", monitoring_enabled=True)
+
+
+def main():
+    """Run the environment configuration examples."""
+    demonstrate_environment_config()
+    demonstrate_development_vs_production()
+
+    print("\n" + "=" * 60)
+    print("Key takeaways:")
+    print("‚úÖ Environment variables provide easy configuration")
+    print("‚úÖ LoggingSettings allow programmatic control")
+    print("‚úÖ Different configs for dev vs production")
+    print("‚úÖ All settings have sensible defaults")
+    print("‚úÖ Configuration is validated automatically")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/03_output_formats.py b/examples/03_output_formats.py
new file mode 100644
index 0000000..a33b7df
--- /dev/null
+++ b/examples/03_output_formats.py
@@ -0,0 +1,218 @@
+#!/usr/bin/env python3
+"""
+Example 3: Output Formats
+
+This example demonstrates the different output formats available in fapilog:
+- JSON format (production-ready, machine-readable)
+- Pretty console format (development-friendly, human-readable)
+- Auto-detection based on TTY
+"""
+
+import sys
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+def demonstrate_json_format():
+    """Show JSON output format (production-ready)."""
+    print("=== JSON Output Format ===")
+    print("Best for: Production, log aggregation, machine processing")
+    print("-" * 50)
+
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="json",  # Force JSON output
+        queue_enabled=False,  # Disable queue for simpler example
+    )
+    configure_logging(settings=settings)
+
+    # Log some structured events
+    log.info(
+        "User login successful",
+        user_id="user_123",
+        ip_address="192.168.1.100",
+        user_agent="Mozilla/5.0...",
+        login_method="password",
+    )
+
+    log.warning(
+        "Rate limit approaching",
+        endpoint="/api/users",
+        current_requests=95,
+        limit=100,
+        window_seconds=60,
+    )
+
+    log.error(
+        "Database connection failed",
+        database="postgres",
+        host="db.example.com",
+        port=5432,
+        error="connection timeout",
+        retry_count=3,
+    )
+
+    print("\nJSON format features:")
+    print("‚úÖ Machine-readable for log aggregation")
+    print("‚úÖ Consistent structure for parsing")
+    print("‚úÖ All fields are properly escaped")
+    print("‚úÖ Suitable for ELK, Loki, Cloud Logging")
+
+
+def demonstrate_pretty_format():
+    """Show pretty console format (development-friendly)."""
+    print("\n\n=== Pretty Console Format ===")
+    print("Best for: Development, debugging, human readability")
+    print("-" * 50)
+
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",  # Force pretty output
+        queue_enabled=False,  # Disable queue for simpler example
+    )
+    configure_logging(settings=settings)
+
+    # Log the same events for comparison
+    log.info(
+        "User login successful",
+        user_id="user_123",
+        ip_address="192.168.1.100",
+        user_agent="Mozilla/5.0...",
+        login_method="password",
+    )
+
+    log.warning(
+        "Rate limit approaching",
+        endpoint="/api/users",
+        current_requests=95,
+        limit=100,
+        window_seconds=60,
+    )
+
+    log.error(
+        "Database connection failed",
+        database="postgres",
+        host="db.example.com",
+        port=5432,
+        error="connection timeout",
+        retry_count=3,
+    )
+
+    print("\nPretty format features:")
+    print("‚úÖ Color-coded log levels")
+    print("‚úÖ Human-readable structure")
+    print("‚úÖ Easy to scan and debug")
+    print("‚úÖ Great for development and testing")
+
+
+def demonstrate_auto_detection():
+    """Show auto-detection based on TTY."""
+    print("\n\n=== Auto-Detection Format ===")
+    print("Best for: Automatic dev/prod switching")
+    print("-" * 50)
+
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="auto",  # Auto-detect based on TTY
+        queue_enabled=False,  # Disable queue for simpler example
+    )
+    configure_logging(settings=settings)
+
+    # Log some events
+    log.info(
+        "Auto-detection example",
+        format="auto",
+        tty_available=hasattr(sys.stderr, "isatty"),
+        detected_format="pretty" if sys.stderr.isatty() else "json",
+    )
+
+    log.info(
+        "This format is automatically chosen",
+        reason="TTY detection",
+        development=sys.stderr.isatty(),
+    )
+
+    print("\nAuto-detection behavior:")
+    print("‚úÖ TTY available ‚Üí Pretty format")
+    print("‚úÖ No TTY (pipes, files) ‚Üí JSON format")
+    print("‚úÖ Perfect for scripts and CI/CD")
+    print("‚úÖ No configuration needed")
+
+
+def demonstrate_complex_structures():
+    """Show how complex data structures are handled."""
+    print("\n\n=== Complex Data Structures ===")
+    print("Demonstrating nested objects, lists, and special values")
+    print("-" * 50)
+
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",  # Use pretty for readability
+        queue_enabled=False,
+    )
+    configure_logging(settings=settings)
+
+    # Nested dictionary
+    user_data = {
+        "profile": {
+            "name": "John Doe",
+            "email": "john@example.com",
+            "preferences": {"theme": "dark", "language": "en"},
+        },
+        "permissions": ["read", "write", "admin"],
+    }
+
+    log.info(
+        "User data processed",
+        user_id="user_456",
+        data=user_data,
+        processing_time_ms=125.5,
+    )
+
+    # List of objects
+    api_calls = [
+        {"endpoint": "/api/users", "status": 200, "duration_ms": 45},
+        {"endpoint": "/api/posts", "status": 200, "duration_ms": 32},
+        {"endpoint": "/api/comments", "status": 404, "duration_ms": 12},
+    ]
+
+    log.info(
+        "API calls summary",
+        total_calls=len(api_calls),
+        successful_calls=2,
+        failed_calls=1,
+        calls=api_calls,
+    )
+
+    # Special values
+    log.info(
+        "Special values example",
+        null_value=None,
+        boolean_true=True,
+        boolean_false=False,
+        empty_string="",
+        zero_number=0,
+        negative_number=-42,
+    )
+
+
+def main():
+    """Run all output format examples."""
+    import sys
+
+    demonstrate_json_format()
+    demonstrate_pretty_format()
+    demonstrate_auto_detection()
+    demonstrate_complex_structures()
+
+    print("\n" + "=" * 60)
+    print("Output Format Summary:")
+    print("‚úÖ JSON: Production, aggregation, machine-readable")
+    print("‚úÖ Pretty: Development, debugging, human-readable")
+    print("‚úÖ Auto: Smart detection based on environment")
+    print("‚úÖ Both handle complex data structures")
+    print("‚úÖ Both preserve all structured data")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/04_logging_levels.py b/examples/04_logging_levels.py
new file mode 100644
index 0000000..3037ae0
--- /dev/null
+++ b/examples/04_logging_levels.py
@@ -0,0 +1,274 @@
+#!/usr/bin/env python3
+"""
+Example 4: Logging Levels
+
+This example demonstrates the different logging levels available in fapilog:
+- DEBUG: Detailed information for debugging
+- INFO: General information about program execution
+- WARNING: Warning messages for potentially problematic situations
+- ERROR: Error messages for serious problems
+- CRITICAL: Critical errors that may prevent the program from running
+"""
+
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+def demonstrate_debug_level():
+    """Show DEBUG level logging."""
+    print("=== DEBUG Level Logging ===")
+    print("Shows: All messages (DEBUG, INFO, WARNING, ERROR, CRITICAL)")
+    print("-" * 50)
+
+    settings = LoggingSettings(
+        level="DEBUG",
+        json_console="pretty",  # Use pretty for readability
+        queue_enabled=False,
+    )
+    configure_logging(settings=settings)
+
+    # All levels will be shown
+    log.debug(
+        "Debug: Detailed debugging information",
+        debug_data="internal state",
+        function="process_user_data",
+        step="validation",
+    )
+
+    log.info("Info: General information", user_id="user_123", action="login")
+
+    log.warning(
+        "Warning: Potentially problematic situation",
+        endpoint="/api/users",
+        rate_limit=95,
+    )
+
+    log.error(
+        "Error: Serious problem occurred",
+        database="postgres",
+        error="connection timeout",
+    )
+
+    log.critical(
+        "Critical: Critical error that may prevent operation",
+        component="database",
+        error="all connections failed",
+    )
+
+
+def demonstrate_info_level():
+    """Show INFO level logging (default)."""
+    print("\n\n=== INFO Level Logging (Default) ===")
+    print("Shows: INFO, WARNING, ERROR, CRITICAL (hides DEBUG)")
+    print("-" * 50)
+
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",
+        queue_enabled=False,
+    )
+    configure_logging(settings=settings)
+
+    # DEBUG will be hidden, others will show
+    log.debug("Debug: This will be hidden", hidden=True)
+    log.info("Info: This will be shown", visible=True)
+    log.warning("Warning: This will be shown", visible=True)
+    log.error("Error: This will be shown", visible=True)
+    log.critical("Critical: This will be shown", visible=True)
+
+
+def demonstrate_warning_level():
+    """Show WARNING level logging."""
+    print("\n\n=== WARNING Level Logging ===")
+    print("Shows: WARNING, ERROR, CRITICAL (hides DEBUG, INFO)")
+    print("-" * 50)
+
+    settings = LoggingSettings(
+        level="WARNING",
+        json_console="pretty",
+        queue_enabled=False,
+    )
+    configure_logging(settings=settings)
+
+    # DEBUG and INFO will be hidden
+    log.debug("Debug: This will be hidden", hidden=True)
+    log.info("Info: This will be hidden", hidden=True)
+    log.warning("Warning: This will be shown", visible=True)
+    log.error("Error: This will be shown", visible=True)
+    log.critical("Critical: This will be shown", visible=True)
+
+
+def demonstrate_error_level():
+    """Show ERROR level logging."""
+    print("\n\n=== ERROR Level Logging ===")
+    print("Shows: ERROR, CRITICAL (hides DEBUG, INFO, WARNING)")
+    print("-" * 50)
+
+    settings = LoggingSettings(
+        level="ERROR",
+        json_console="pretty",
+        queue_enabled=False,
+    )
+    configure_logging(settings=settings)
+
+    # Only ERROR and CRITICAL will show
+    log.debug("Debug: This will be hidden", hidden=True)
+    log.info("Info: This will be hidden", hidden=True)
+    log.warning("Warning: This will be hidden", hidden=True)
+    log.error("Error: This will be shown", visible=True)
+    log.critical("Critical: This will be shown", visible=True)
+
+
+def demonstrate_critical_level():
+    """Show CRITICAL level logging."""
+    print("\n\n=== CRITICAL Level Logging ===")
+    print("Shows: Only CRITICAL (hides all others)")
+    print("-" * 50)
+
+    settings = LoggingSettings(
+        level="CRITICAL",
+        json_console="pretty",
+        queue_enabled=False,
+    )
+    configure_logging(settings=settings)
+
+    # Only CRITICAL will show
+    log.debug("Debug: This will be hidden", hidden=True)
+    log.info("Info: This will be hidden", hidden=True)
+    log.warning("Warning: This will be hidden", hidden=True)
+    log.error("Error: This will be hidden", hidden=True)
+    log.critical("Critical: Only this will be shown", visible=True)
+
+
+def demonstrate_practical_examples():
+    """Show practical examples of when to use each level."""
+    print("\n\n=== Practical Examples ===")
+    print("Real-world scenarios for each logging level")
+    print("-" * 50)
+
+    settings = LoggingSettings(
+        level="DEBUG",  # Show all levels for this example
+        json_console="pretty",
+        queue_enabled=False,
+    )
+    configure_logging(settings=settings)
+
+    # DEBUG examples
+    log.debug(
+        "Function entry",
+        function="process_payment",
+        parameters={"amount": 100, "currency": "USD"},
+    )
+
+    log.debug(
+        "Database query executed",
+        query="SELECT * FROM users WHERE id = ?",
+        parameters=[123],
+        execution_time_ms=5.2,
+    )
+
+    # INFO examples
+    log.info(
+        "User action completed",
+        user_id="user_456",
+        action="purchase",
+        amount=99.99,
+        currency="USD",
+    )
+
+    log.info("Service started", service="payment_processor", version="1.2.3", port=8080)
+
+    # WARNING examples
+    log.warning(
+        "Rate limit approaching",
+        endpoint="/api/payments",
+        current_requests=95,
+        limit=100,
+    )
+
+    log.warning(
+        "Deprecated API used",
+        endpoint="/api/v1/users",
+        replacement="/api/v2/users",
+        sunset_date="2024-12-31",
+    )
+
+    # ERROR examples
+    log.error(
+        "Database connection failed",
+        database="postgres",
+        host="db.example.com",
+        error="connection timeout",
+        retry_count=3,
+    )
+
+    log.error(
+        "Payment processing failed",
+        payment_id="pay_123",
+        error="insufficient_funds",
+        user_id="user_789",
+    )
+
+    # CRITICAL examples
+    log.critical(
+        "All database connections failed",
+        database="postgres",
+        error="connection pool exhausted",
+        impact="service unavailable",
+    )
+
+    log.critical(
+        "Application startup failed",
+        error="configuration invalid",
+        missing_vars=["DATABASE_URL", "API_KEY"],
+    )
+
+
+def demonstrate_level_comparison():
+    """Show a comparison of all levels side by side."""
+    print("\n\n=== Logging Level Comparison ===")
+    print("Summary of what each level shows:")
+    print("-" * 50)
+
+    levels = [
+        ("DEBUG", "All messages"),
+        ("INFO", "INFO, WARNING, ERROR, CRITICAL"),
+        ("WARNING", "WARNING, ERROR, CRITICAL"),
+        ("ERROR", "ERROR, CRITICAL"),
+        ("CRITICAL", "CRITICAL only"),
+    ]
+
+    for level, description in levels:
+        print(f"{level:8} ‚Üí {description}")
+
+    print("\nRecommended usage:")
+    print("DEBUG    ‚Üí Development, troubleshooting")
+    print("INFO     ‚Üí Production (default)")
+    print("WARNING  ‚Üí Production with reduced noise")
+    print("ERROR    ‚Üí Error monitoring only")
+    print("CRITICAL ‚Üí Emergency situations only")
+
+
+def main():
+    """Run all logging level examples."""
+    demonstrate_debug_level()
+    demonstrate_info_level()
+    demonstrate_warning_level()
+    demonstrate_error_level()
+    demonstrate_critical_level()
+    demonstrate_practical_examples()
+    demonstrate_level_comparison()
+
+    print("\n" + "=" * 60)
+    print("Logging Level Summary:")
+    print("‚úÖ DEBUG: Detailed debugging information")
+    print("‚úÖ INFO: General information (default)")
+    print("‚úÖ WARNING: Potentially problematic situations")
+    print("‚úÖ ERROR: Serious problems")
+    print("‚úÖ CRITICAL: Critical errors that may prevent operation")
+    print("‚úÖ Levels are hierarchical (higher levels include lower)")
+    print("‚úÖ Use appropriate level for your use case")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/05_fastapi_basic.py b/examples/05_fastapi_basic.py
new file mode 100644
index 0000000..38cb290
--- /dev/null
+++ b/examples/05_fastapi_basic.py
@@ -0,0 +1,217 @@
+#!/usr/bin/env python3
+"""
+Example 5: FastAPI Basic Integration
+
+This example demonstrates how to integrate fapilog with a FastAPI application.
+It shows basic setup, middleware integration, and structured logging in API endpoints.
+"""
+
+import asyncio
+import time
+from typing import Dict, List
+
+from fastapi import FastAPI, HTTPException, Request
+from fastapi.responses import JSONResponse
+
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+def create_app() -> FastAPI:
+    """Create a FastAPI application with fapilog integration."""
+
+    # Configure logging with FastAPI integration
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",  # Pretty output for development
+        queue_enabled=False,  # Disable queue for simpler example
+    )
+
+    # Configure logging and get the app instance
+    configure_logging(settings=settings)
+
+    # Create FastAPI app
+    app = FastAPI(
+        title="fapilog Example API",
+        description="Example FastAPI application with structured logging",
+        version="1.0.0",
+    )
+
+    # Log application startup
+    log.info(
+        "FastAPI application starting",
+        app_name="fapilog-example",
+        version="1.0.0",
+        environment="development",
+    )
+
+    return app
+
+
+# Create the FastAPI application
+app = create_app()
+
+
+@app.get("/")
+async def root():
+    """Root endpoint with basic logging."""
+    log.info("Root endpoint accessed")
+    return {"message": "Hello from fapilog example API"}
+
+
+@app.get("/users")
+async def get_users():
+    """Get users endpoint with structured logging."""
+    # Simulate some processing
+    await asyncio.sleep(0.1)
+
+    # Log the request with structured data
+    log.info("Users endpoint accessed", endpoint="/users", method="GET", user_count=3)
+
+    users = [
+        {"id": 1, "name": "Alice", "email": "alice@example.com"},
+        {"id": 2, "name": "Bob", "email": "bob@example.com"},
+        {"id": 3, "name": "Charlie", "email": "charlie@example.com"},
+    ]
+
+    return {"users": users}
+
+
+@app.get("/users/{user_id}")
+async def get_user(user_id: int):
+    """Get specific user with error handling and logging."""
+    log.info(
+        "User detail requested",
+        endpoint=f"/users/{user_id}",
+        method="GET",
+        user_id=user_id,
+    )
+
+    # Simulate database lookup
+    await asyncio.sleep(0.05)
+
+    if user_id == 1:
+        user = {"id": 1, "name": "Alice", "email": "alice@example.com"}
+        log.info("User found", user_id=user_id, user_name=user["name"])
+        return user
+    elif user_id == 2:
+        user = {"id": 2, "name": "Bob", "email": "bob@example.com"}
+        log.info("User found", user_id=user_id, user_name=user["name"])
+        return user
+    else:
+        log.warning("User not found", user_id=user_id, error="user_not_found")
+        raise HTTPException(status_code=404, detail="User not found")
+
+
+@app.post("/users")
+async def create_user(request: Request):
+    """Create user endpoint with request body logging."""
+    # Get request body
+    body = await request.json()
+
+    log.info(
+        "User creation requested", endpoint="/users", method="POST", user_data=body
+    )
+
+    # Simulate validation
+    if "name" not in body or "email" not in body:
+        log.error(
+            "Invalid user data",
+            endpoint="/users",
+            method="POST",
+            error="missing_required_fields",
+            provided_fields=list(body.keys()),
+        )
+        raise HTTPException(status_code=400, detail="Name and email are required")
+
+    # Simulate database operation
+    await asyncio.sleep(0.1)
+
+    new_user = {"id": 4, "name": body["name"], "email": body["email"]}
+
+    log.info(
+        "User created successfully",
+        endpoint="/users",
+        method="POST",
+        user_id=new_user["id"],
+        user_name=new_user["name"],
+    )
+
+    return new_user
+
+
+@app.get("/health")
+async def health_check():
+    """Health check endpoint with minimal logging."""
+    log.info("Health check requested")
+    return {"status": "healthy", "timestamp": time.time()}
+
+
+@app.get("/slow")
+async def slow_endpoint():
+    """Slow endpoint to demonstrate timing logs."""
+    log.info("Slow endpoint started")
+
+    # Simulate slow operation
+    await asyncio.sleep(2)
+
+    log.info("Slow endpoint completed")
+    return {"message": "Slow operation completed"}
+
+
+@app.get("/error")
+async def error_endpoint():
+    """Endpoint that always raises an error."""
+    log.error(
+        "Error endpoint accessed - this is expected",
+        endpoint="/error",
+        method="GET",
+        error_type="simulated_error",
+    )
+
+    raise HTTPException(status_code=500, detail="This is a simulated error")
+
+
+# Custom exception handler to log errors
+@app.exception_handler(Exception)
+async def global_exception_handler(request: Request, exc: Exception):
+    """Global exception handler with logging."""
+    log.error(
+        "Unhandled exception occurred",
+        endpoint=str(request.url.path),
+        method=request.method,
+        error=str(exc),
+        error_type=type(exc).__name__,
+    )
+
+    return JSONResponse(status_code=500, content={"detail": "Internal server error"})
+
+
+def main():
+    """Run the FastAPI application."""
+    import uvicorn
+
+    print("=== FastAPI Basic Integration Example ===")
+    print("Starting FastAPI application with fapilog...")
+    print("Available endpoints:")
+    print("  GET  /         - Root endpoint")
+    print("  GET  /users    - List users")
+    print("  GET  /users/{id} - Get specific user")
+    print("  POST /users    - Create user")
+    print("  GET  /health   - Health check")
+    print("  GET  /slow     - Slow endpoint")
+    print("  GET  /error    - Error endpoint")
+    print()
+    print("Start the server with: uvicorn examples.05_fastapi_basic:app --reload")
+    print("Then visit: http://localhost:8000/docs")
+    print()
+    print("Key features demonstrated:")
+    print("‚úÖ FastAPI integration with middleware")
+    print("‚úÖ Structured logging in endpoints")
+    print("‚úÖ Error handling and logging")
+    print("‚úÖ Request/response correlation")
+    print("‚úÖ Automatic trace ID generation")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/06_fastapi_middleware.py b/examples/06_fastapi_middleware.py
new file mode 100644
index 0000000..2de8f79
--- /dev/null
+++ b/examples/06_fastapi_middleware.py
@@ -0,0 +1,259 @@
+#!/usr/bin/env python3
+"""
+Example 6: FastAPI Middleware Integration
+
+This example demonstrates the TraceIDMiddleware integration and shows how
+trace IDs, span IDs, and request metadata are automatically captured and
+propagated through the logging system.
+"""
+
+import asyncio
+import time
+from typing import Dict, Optional
+
+from fastapi import FastAPI, HTTPException, Request, Response
+from fastapi.responses import JSONResponse
+
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+def create_app() -> FastAPI:
+    """Create a FastAPI application with middleware integration."""
+
+    # Configure logging with middleware support
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",  # Pretty output for development
+        queue_enabled=False,  # Disable queue for simpler example
+    )
+
+    # Configure logging - this automatically registers TraceIDMiddleware
+    configure_logging(settings=settings)
+
+    # Create FastAPI app
+    app = FastAPI(
+        title="fapilog Middleware Example",
+        description="Example showing trace ID middleware and request correlation",
+        version="1.0.0",
+    )
+
+    # Log application startup
+    log.info(
+        "FastAPI application with middleware starting",
+        app_name="fapilog-middleware-example",
+        version="1.0.0",
+        middleware_enabled=True,
+    )
+
+    return app
+
+
+# Create the FastAPI application
+app = create_app()
+
+
+@app.get("/")
+async def root():
+    """Root endpoint - trace ID will be automatically added to logs."""
+    log.info("Root endpoint accessed")
+    return {"message": "Hello from fapilog middleware example"}
+
+
+@app.get("/trace")
+async def trace_example():
+    """Endpoint to demonstrate trace ID propagation."""
+    log.info("Trace example endpoint accessed")
+
+    # Simulate some work
+    await asyncio.sleep(0.1)
+
+    log.info("Processing trace example", step="validation")
+
+    await asyncio.sleep(0.05)
+
+    log.info("Trace example completed", step="completion")
+
+    return {
+        "message": "Trace example completed",
+        "note": "Check the logs to see trace_id and span_id fields",
+    }
+
+
+@app.get("/nested")
+async def nested_calls():
+    """Endpoint that demonstrates nested function calls with trace propagation."""
+    log.info("Nested calls endpoint accessed")
+
+    # Call helper functions that also log
+    result1 = await helper_function_1()
+    result2 = await helper_function_2()
+
+    log.info("Nested calls completed", result1=result1, result2=result2)
+
+    return {"message": "Nested calls completed", "results": [result1, result2]}
+
+
+async def helper_function_1() -> str:
+    """Helper function that logs with trace context."""
+    log.info("Helper function 1 called")
+    await asyncio.sleep(0.05)
+    log.info("Helper function 1 completed")
+    return "helper_1_result"
+
+
+async def helper_function_2() -> str:
+    """Another helper function that logs with trace context."""
+    log.info("Helper function 2 called")
+    await asyncio.sleep(0.03)
+    log.info("Helper function 2 completed")
+    return "helper_2_result"
+
+
+@app.get("/headers")
+async def check_headers(request: Request):
+    """Endpoint to check request headers and demonstrate header propagation."""
+    # Log request headers (some may be redacted for security)
+    log.info(
+        "Headers endpoint accessed",
+        user_agent=request.headers.get("user-agent", "unknown"),
+        content_type=request.headers.get("content-type", "none"),
+    )
+
+    return {
+        "message": "Headers logged",
+        "user_agent": request.headers.get("user-agent", "unknown"),
+        "content_type": request.headers.get("content-type", "none"),
+    }
+
+
+@app.post("/data")
+async def process_data(request: Request):
+    """Endpoint that processes request data and logs body size."""
+    # Get request body
+    body = await request.json()
+
+    log.info(
+        "Data processing started",
+        data_size=len(str(body)),
+        data_keys=list(body.keys()) if isinstance(body, dict) else "not_dict",
+    )
+
+    # Simulate processing
+    await asyncio.sleep(0.1)
+
+    # Process the data
+    processed_data = {"original": body, "processed": True, "timestamp": time.time()}
+
+    log.info(
+        "Data processing completed",
+        original_size=len(str(body)),
+        processed_size=len(str(processed_data)),
+    )
+
+    return processed_data
+
+
+@app.get("/error-with-trace")
+async def error_with_trace():
+    """Endpoint that raises an error to show trace ID in error logs."""
+    log.error(
+        "About to raise an error - this is expected",
+        error_type="simulated_error",
+        step="before_error",
+    )
+
+    # This will raise an error, but the trace ID will still be in the logs
+    raise HTTPException(status_code=500, detail="Simulated error with trace ID")
+
+
+@app.get("/slow-with-trace")
+async def slow_with_trace():
+    """Slow endpoint to demonstrate timing with trace IDs."""
+    log.info("Slow operation started")
+
+    # Simulate slow operation
+    await asyncio.sleep(2)
+
+    log.info("Slow operation completed")
+
+    return {"message": "Slow operation completed - check logs for timing"}
+
+
+@app.get("/correlation")
+async def correlation_example():
+    """Endpoint to demonstrate request correlation across multiple logs."""
+    request_id = f"req_{int(time.time())}"
+
+    log.info("Correlation example started", request_id=request_id, step="start")
+
+    # Simulate multiple operations
+    for i in range(3):
+        await asyncio.sleep(0.1)
+        log.info(
+            f"Operation {i+1} completed", request_id=request_id, step=f"operation_{i+1}"
+        )
+
+    log.info("Correlation example completed", request_id=request_id, step="complete")
+
+    return {
+        "message": "Correlation example completed",
+        "request_id": request_id,
+        "note": "All logs for this request will have the same trace_id",
+    }
+
+
+# Custom middleware to demonstrate additional header handling
+@app.middleware("http")
+async def custom_header_middleware(request: Request, call_next):
+    """Custom middleware to add additional headers."""
+    # Add custom header
+    request.state.custom_header = "custom_value"
+
+    # Process the request
+    response = await call_next(request)
+
+    # Add custom response header
+    response.headers["X-Custom-Header"] = "custom_response_value"
+
+    return response
+
+
+def main():
+    """Run the FastAPI application."""
+    import uvicorn
+
+    print("=== FastAPI Middleware Integration Example ===")
+    print("This example demonstrates:")
+    print("‚úÖ Automatic trace ID generation")
+    print("‚úÖ Span ID for each request")
+    print("‚úÖ Request/response timing")
+    print("‚úÖ Header propagation")
+    print("‚úÖ Body size tracking")
+    print("‚úÖ Error correlation")
+    print()
+    print("Available endpoints:")
+    print("  GET  /              - Basic endpoint")
+    print("  GET  /trace         - Trace ID demonstration")
+    print("  GET  /nested        - Nested function calls")
+    print("  GET  /headers       - Header logging")
+    print("  POST /data          - Request body processing")
+    print("  GET  /error-with-trace - Error with trace ID")
+    print("  GET  /slow-with-trace  - Slow operation timing")
+    print("  GET  /correlation   - Request correlation")
+    print()
+    print("Start the server with: uvicorn examples.06_fastapi_middleware:app --reload")
+    print("Then visit: http://localhost:8000/docs")
+    print()
+    print("Key features to observe in logs:")
+    print("‚úÖ trace_id: Unique identifier for each request")
+    print("‚úÖ span_id: Unique identifier for each request span")
+    print("‚úÖ latency_ms: Request processing time")
+    print("‚úÖ req_bytes: Request body size")
+    print("‚úÖ res_bytes: Response body size")
+    print("‚úÖ status_code: HTTP status code")
+    print("‚úÖ user_agent: User agent string")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/07_fastapi_error_handling.py b/examples/07_fastapi_error_handling.py
new file mode 100644
index 0000000..b99cae6
--- /dev/null
+++ b/examples/07_fastapi_error_handling.py
@@ -0,0 +1,483 @@
+#!/usr/bin/env python3
+"""
+Example 7: FastAPI Error Handling and Logging
+
+This example demonstrates comprehensive error handling and logging in FastAPI
+applications, including custom exceptions, validation errors, and proper
+error correlation with trace IDs.
+"""
+
+import asyncio
+import time
+from typing import Dict, List, Optional, Union
+
+from fastapi import FastAPI, HTTPException, Request, Response
+from fastapi.exceptions import RequestValidationError
+from fastapi.responses import JSONResponse
+from pydantic import BaseModel, ValidationError
+
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+# Custom exception classes
+class BusinessLogicError(Exception):
+    """Custom exception for business logic errors."""
+
+    def __init__(self, message: str, error_code: str, details: Optional[Dict] = None):
+        self.message = message
+        self.error_code = error_code
+        self.details = details or {}
+        super().__init__(message)
+
+
+class DatabaseError(Exception):
+    """Custom exception for database errors."""
+
+    def __init__(self, message: str, operation: str, table: str):
+        self.message = message
+        self.operation = operation
+        self.table = table
+        super().__init__(message)
+
+
+# Pydantic models for validation
+class UserCreate(BaseModel):
+    name: str
+    email: str
+    age: Optional[int] = None
+
+
+class UserUpdate(BaseModel):
+    name: Optional[str] = None
+    email: Optional[str] = None
+    age: Optional[int] = None
+
+
+def create_app() -> FastAPI:
+    """Create a FastAPI application with comprehensive error handling."""
+
+    # Configure logging
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",
+        queue_enabled=False,
+    )
+
+    configure_logging(settings=settings)
+
+    # Create FastAPI app
+    app = FastAPI(
+        title="fapilog Error Handling Example",
+        description="Example showing comprehensive error handling and logging",
+        version="1.0.0",
+    )
+
+    log.info(
+        "FastAPI application with error handling starting",
+        app_name="fapilog-error-handling-example",
+        version="1.0.0",
+    )
+
+    return app
+
+
+# Create the FastAPI application
+app = create_app()
+
+
+@app.get("/")
+async def root():
+    """Root endpoint."""
+    log.info("Root endpoint accessed")
+    return {"message": "Error handling example API"}
+
+
+@app.post("/users")
+async def create_user(user_data: UserCreate):
+    """Create user with validation and error handling."""
+    log.info(
+        "User creation requested", user_name=user_data.name, user_email=user_data.email
+    )
+
+    try:
+        # Simulate validation
+        if user_data.age and user_data.age < 0:
+            log.warning(
+                "Invalid age provided", user_name=user_data.name, age=user_data.age
+            )
+            raise HTTPException(status_code=400, detail="Age must be positive")
+
+        # Simulate database operation
+        await asyncio.sleep(0.1)
+
+        # Simulate duplicate email check
+        if user_data.email == "existing@example.com":
+            log.error(
+                "Duplicate email attempted",
+                email=user_data.email,
+                error_code="DUPLICATE_EMAIL",
+            )
+            raise BusinessLogicError(
+                message="Email already exists",
+                error_code="DUPLICATE_EMAIL",
+                details={"email": user_data.email},
+            )
+
+        # Simulate database error
+        if user_data.name == "error":
+            log.error(
+                "Database error simulated", operation="create_user", table="users"
+            )
+            raise DatabaseError(
+                message="Database connection failed",
+                operation="create_user",
+                table="users",
+            )
+
+        new_user = {
+            "id": 1,
+            "name": user_data.name,
+            "email": user_data.email,
+            "age": user_data.age,
+        }
+
+        log.info(
+            "User created successfully",
+            user_id=new_user["id"],
+            user_name=new_user["name"],
+        )
+
+        return new_user
+
+    except HTTPException:
+        # Re-raise HTTP exceptions
+        raise
+    except BusinessLogicError as e:
+        log.error(
+            "Business logic error",
+            error_code=e.error_code,
+            message=e.message,
+            details=e.details,
+        )
+        raise HTTPException(
+            status_code=400,
+            detail={
+                "error": e.message,
+                "error_code": e.error_code,
+                "details": e.details,
+            },
+        )
+    except DatabaseError as e:
+        log.error(
+            "Database error", operation=e.operation, table=e.table, message=e.message
+        )
+        raise HTTPException(
+            status_code=500,
+            detail={
+                "error": "Database operation failed",
+                "operation": e.operation,
+                "table": e.table,
+            },
+        )
+    except Exception as e:
+        log.error(
+            "Unexpected error during user creation",
+            error=str(e),
+            error_type=type(e).__name__,
+        )
+        raise HTTPException(status_code=500, detail="Internal server error")
+
+
+@app.get("/users/{user_id}")
+async def get_user(user_id: int):
+    """Get user with error handling."""
+    log.info("User retrieval requested", user_id=user_id)
+
+    try:
+        # Simulate database lookup
+        await asyncio.sleep(0.05)
+
+        if user_id == 1:
+            user = {"id": 1, "name": "Alice", "email": "alice@example.com"}
+            log.info("User found", user_id=user_id, user_name=user["name"])
+            return user
+        elif user_id == 999:
+            # Simulate database error
+            log.error(
+                "Database error for user lookup",
+                user_id=user_id,
+                operation="get_user",
+                table="users",
+            )
+            raise DatabaseError(
+                message="Database connection timeout",
+                operation="get_user",
+                table="users",
+            )
+        else:
+            log.warning("User not found", user_id=user_id)
+            raise HTTPException(status_code=404, detail="User not found")
+
+    except HTTPException:
+        raise
+    except DatabaseError as e:
+        log.error(
+            "Database error during user retrieval",
+            user_id=user_id,
+            operation=e.operation,
+            table=e.table,
+            message=e.message,
+        )
+        raise HTTPException(status_code=500, detail="Database error")
+    except Exception as e:
+        log.error(
+            "Unexpected error during user retrieval",
+            user_id=user_id,
+            error=str(e),
+            error_type=type(e).__name__,
+        )
+        raise HTTPException(status_code=500, detail="Internal server error")
+
+
+@app.put("/users/{user_id}")
+async def update_user(user_id: int, user_data: UserUpdate):
+    """Update user with validation and error handling."""
+    log.info(
+        "User update requested",
+        user_id=user_id,
+        update_fields=list(user_data.dict(exclude_unset=True).keys()),
+    )
+
+    try:
+        # Simulate validation
+        if user_data.age is not None and user_data.age < 0:
+            log.warning("Invalid age in update", user_id=user_id, age=user_data.age)
+            raise HTTPException(status_code=400, detail="Age must be positive")
+
+        # Simulate database operation
+        await asyncio.sleep(0.1)
+
+        if user_id == 999:
+            raise DatabaseError(
+                message="Database connection failed",
+                operation="update_user",
+                table="users",
+            )
+
+        updated_user = {
+            "id": user_id,
+            "name": user_data.name or "Updated User",
+            "email": user_data.email or "updated@example.com",
+            "age": user_data.age,
+        }
+
+        log.info("User updated successfully", user_id=user_id)
+
+        return updated_user
+
+    except HTTPException:
+        raise
+    except DatabaseError as e:
+        log.error(
+            "Database error during user update",
+            user_id=user_id,
+            operation=e.operation,
+            table=e.table,
+            message=e.message,
+        )
+        raise HTTPException(status_code=500, detail="Database error")
+    except Exception as e:
+        log.error(
+            "Unexpected error during user update",
+            user_id=user_id,
+            error=str(e),
+            error_type=type(e).__name__,
+        )
+        raise HTTPException(status_code=500, detail="Internal server error")
+
+
+@app.delete("/users/{user_id}")
+async def delete_user(user_id: int):
+    """Delete user with error handling."""
+    log.info("User deletion requested", user_id=user_id)
+
+    try:
+        # Simulate database operation
+        await asyncio.sleep(0.1)
+
+        if user_id == 1:
+            log.info("User deleted successfully", user_id=user_id)
+            return {"message": "User deleted successfully"}
+        elif user_id == 999:
+            raise DatabaseError(
+                message="Database connection failed",
+                operation="delete_user",
+                table="users",
+            )
+        else:
+            log.warning("User not found for deletion", user_id=user_id)
+            raise HTTPException(status_code=404, detail="User not found")
+
+    except HTTPException:
+        raise
+    except DatabaseError as e:
+        log.error(
+            "Database error during user deletion",
+            user_id=user_id,
+            operation=e.operation,
+            table=e.table,
+            message=e.message,
+        )
+        raise HTTPException(status_code=500, detail="Database error")
+    except Exception as e:
+        log.error(
+            "Unexpected error during user deletion",
+            user_id=user_id,
+            error=str(e),
+            error_type=type(e).__name__,
+        )
+        raise HTTPException(status_code=500, detail="Internal server error")
+
+
+@app.get("/error-simulation")
+async def error_simulation():
+    """Endpoint to simulate different types of errors."""
+    log.info("Error simulation endpoint accessed")
+
+    # Simulate different error types
+    error_type = "validation"  # Change this to test different errors
+
+    if error_type == "validation":
+        log.error("Validation error simulated")
+        raise HTTPException(status_code=422, detail="Validation error")
+    elif error_type == "business":
+        log.error("Business logic error simulated")
+        raise BusinessLogicError(
+            message="Business rule violation",
+            error_code="BUSINESS_RULE_VIOLATION",
+            details={"rule": "example_rule"},
+        )
+    elif error_type == "database":
+        log.error("Database error simulated")
+        raise DatabaseError(
+            message="Database connection failed",
+            operation="simulate_error",
+            table="test",
+        )
+    else:
+        log.error("Generic error simulated")
+        raise Exception("Generic error for testing")
+
+
+# Custom exception handlers
+@app.exception_handler(RequestValidationError)
+async def validation_exception_handler(request: Request, exc: RequestValidationError):
+    """Handle Pydantic validation errors."""
+    log.error(
+        "Request validation error",
+        endpoint=str(request.url.path),
+        method=request.method,
+        errors=exc.errors(),
+    )
+
+    return JSONResponse(
+        status_code=422, content={"detail": "Validation error", "errors": exc.errors()}
+    )
+
+
+@app.exception_handler(BusinessLogicError)
+async def business_logic_exception_handler(request: Request, exc: BusinessLogicError):
+    """Handle business logic errors."""
+    log.error(
+        "Business logic error",
+        endpoint=str(request.url.path),
+        method=request.method,
+        error_code=exc.error_code,
+        message=exc.message,
+        details=exc.details,
+    )
+
+    return JSONResponse(
+        status_code=400,
+        content={
+            "detail": exc.message,
+            "error_code": exc.error_code,
+            "details": exc.details,
+        },
+    )
+
+
+@app.exception_handler(DatabaseError)
+async def database_exception_handler(request: Request, exc: DatabaseError):
+    """Handle database errors."""
+    log.error(
+        "Database error",
+        endpoint=str(request.url.path),
+        method=request.method,
+        operation=exc.operation,
+        table=exc.table,
+        message=exc.message,
+    )
+
+    return JSONResponse(
+        status_code=500,
+        content={
+            "detail": "Database operation failed",
+            "operation": exc.operation,
+            "table": exc.table,
+        },
+    )
+
+
+@app.exception_handler(Exception)
+async def generic_exception_handler(request: Request, exc: Exception):
+    """Handle all other exceptions."""
+    log.error(
+        "Unhandled exception",
+        endpoint=str(request.url.path),
+        method=request.method,
+        error=str(exc),
+        error_type=type(exc).__name__,
+    )
+
+    return JSONResponse(status_code=500, content={"detail": "Internal server error"})
+
+
+def main():
+    """Run the FastAPI application."""
+    import uvicorn
+
+    print("=== FastAPI Error Handling Example ===")
+    print("This example demonstrates:")
+    print("‚úÖ Custom exception classes")
+    print("‚úÖ Comprehensive error handling")
+    print("‚úÖ Error logging with context")
+    print("‚úÖ Validation error handling")
+    print("‚úÖ Business logic error handling")
+    print("‚úÖ Database error handling")
+    print("‚úÖ Trace ID correlation in errors")
+    print()
+    print("Available endpoints:")
+    print("  POST /users              - Create user (with validation)")
+    print("  GET  /users/{id}         - Get user (with error handling)")
+    print("  PUT  /users/{id}         - Update user (with validation)")
+    print("  DELETE /users/{id}       - Delete user (with error handling)")
+    print("  GET  /error-simulation   - Simulate different errors")
+    print()
+    print("Test scenarios:")
+    print("‚úÖ Valid user creation: POST /users with valid data")
+    print("‚úÖ Invalid age: POST /users with age < 0")
+    print("‚úÖ Duplicate email: POST /users with email 'existing@example.com'")
+    print("‚úÖ Database error: POST /users with name 'error'")
+    print("‚úÖ User not found: GET /users/999")
+    print("‚úÖ Database error: GET /users/999")
+    print()
+    print(
+        "Start the server with: uvicorn examples.07_fastapi_error_handling:app --reload"
+    )
+    print("Then visit: http://localhost:8000/docs")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/08_fastapi_structured_logging.py b/examples/08_fastapi_structured_logging.py
new file mode 100644
index 0000000..8215b2d
--- /dev/null
+++ b/examples/08_fastapi_structured_logging.py
@@ -0,0 +1,578 @@
+#!/usr/bin/env python3
+"""
+Example 8: FastAPI Structured Logging Patterns
+
+This example demonstrates advanced structured logging patterns in FastAPI
+applications, including performance monitoring, business metrics, and
+comprehensive request/response logging.
+"""
+
+import asyncio
+import time
+import uuid
+from typing import Dict, List, Optional, Any
+from datetime import datetime
+
+from fastapi import FastAPI, HTTPException, Request, Response, Depends
+from fastapi.responses import JSONResponse
+from pydantic import BaseModel
+
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+# Pydantic models
+class Product(BaseModel):
+    id: int
+    name: str
+    price: float
+    category: str
+    in_stock: bool
+
+
+class OrderItem(BaseModel):
+    product_id: int
+    quantity: int
+
+
+class Order(BaseModel):
+    customer_id: int
+    items: List[OrderItem]
+    shipping_address: str
+
+
+class OrderResponse(BaseModel):
+    order_id: str
+    customer_id: int
+    total_amount: float
+    status: str
+    created_at: datetime
+
+
+def create_app() -> FastAPI:
+    """Create a FastAPI application with structured logging patterns."""
+
+    # Configure logging
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",
+        queue_enabled=False,
+    )
+
+    configure_logging(settings=settings)
+
+    # Create FastAPI app
+    app = FastAPI(
+        title="fapilog Structured Logging Example",
+        description="Example showing advanced structured logging patterns",
+        version="1.0.0",
+    )
+
+    log.info(
+        "FastAPI application with structured logging starting",
+        app_name="fapilog-structured-logging-example",
+        version="1.0.0",
+        features=["performance_monitoring", "business_metrics", "request_tracking"],
+    )
+
+    return app
+
+
+# Create the FastAPI application
+app = create_app()
+
+
+# Dependency for request timing
+async def log_request_timing(request: Request):
+    """Dependency to log request timing and metadata."""
+    start_time = time.perf_counter()
+
+    # Log request start
+    log.info(
+        "Request started",
+        endpoint=str(request.url.path),
+        method=request.method,
+        client_ip=request.client.host if request.client else "unknown",
+        user_agent=request.headers.get("user-agent", "unknown"),
+    )
+
+    # Store timing info in request state
+    request.state.start_time = start_time
+
+    return start_time
+
+
+# Dependency for response logging
+async def log_response_timing(request: Request, response: Response):
+    """Dependency to log response timing and metadata."""
+    if hasattr(request.state, "start_time"):
+        duration = time.perf_counter() - request.state.start_time
+        duration_ms = round(duration * 1000, 2)
+
+        log.info(
+            "Request completed",
+            endpoint=str(request.url.path),
+            method=request.method,
+            status_code=response.status_code,
+            duration_ms=duration_ms,
+            response_size=len(str(response.body)) if hasattr(response, "body") else 0,
+        )
+
+
+# Mock data store
+products_db = {
+    1: {
+        "id": 1,
+        "name": "Laptop",
+        "price": 999.99,
+        "category": "Electronics",
+        "in_stock": True,
+    },
+    2: {
+        "id": 2,
+        "name": "Mouse",
+        "price": 29.99,
+        "category": "Electronics",
+        "in_stock": True,
+    },
+    3: {
+        "id": 3,
+        "name": "Keyboard",
+        "price": 79.99,
+        "category": "Electronics",
+        "in_stock": False,
+    },
+    4: {"id": 4, "name": "Book", "price": 19.99, "category": "Books", "in_stock": True},
+}
+
+orders_db = {}
+
+
+@app.get("/")
+async def root():
+    """Root endpoint with basic structured logging."""
+    log.info(
+        "Root endpoint accessed",
+        endpoint="/",
+        method="GET",
+        timestamp=datetime.now().isoformat(),
+    )
+
+    return {"message": "Structured logging example API"}
+
+
+@app.get("/products")
+async def get_products(
+    category: Optional[str] = None,
+    in_stock_only: bool = False,
+    timing: float = Depends(log_request_timing),
+):
+    """Get products with filtering and performance logging."""
+    log.info(
+        "Products request started",
+        endpoint="/products",
+        method="GET",
+        filters={"category": category, "in_stock_only": in_stock_only},
+    )
+
+    # Simulate database query
+    await asyncio.sleep(0.1)
+
+    # Apply filters
+    filtered_products = []
+    for product in products_db.values():
+        if category and product["category"] != category:
+            continue
+        if in_stock_only and not product["in_stock"]:
+            continue
+        filtered_products.append(product)
+
+    # Log performance metrics
+    log.info(
+        "Products retrieved successfully",
+        endpoint="/products",
+        method="GET",
+        total_products=len(products_db),
+        filtered_count=len(filtered_products),
+        filters_applied={
+            "category_filter": category is not None,
+            "stock_filter": in_stock_only,
+        },
+    )
+
+    return {"products": filtered_products}
+
+
+@app.get("/products/{product_id}")
+async def get_product(product_id: int, timing: float = Depends(log_request_timing)):
+    """Get specific product with detailed logging."""
+    log.info(
+        "Product detail requested",
+        endpoint=f"/products/{product_id}",
+        method="GET",
+        product_id=product_id,
+    )
+
+    # Simulate database lookup
+    await asyncio.sleep(0.05)
+
+    if product_id not in products_db:
+        log.warning(
+            "Product not found",
+            endpoint=f"/products/{product_id}",
+            method="GET",
+            product_id=product_id,
+            available_ids=list(products_db.keys()),
+        )
+        raise HTTPException(status_code=404, detail="Product not found")
+
+    product = products_db[product_id]
+
+    log.info(
+        "Product retrieved successfully",
+        endpoint=f"/products/{product_id}",
+        method="GET",
+        product_id=product_id,
+        product_name=product["name"],
+        product_price=product["price"],
+        product_category=product["category"],
+        in_stock=product["in_stock"],
+    )
+
+    return product
+
+
+@app.post("/orders")
+async def create_order(order: Order, timing: float = Depends(log_request_timing)):
+    """Create order with comprehensive business logging."""
+    order_id = str(uuid.uuid4())
+
+    log.info(
+        "Order creation started",
+        endpoint="/orders",
+        method="POST",
+        order_id=order_id,
+        customer_id=order.customer_id,
+        item_count=len(order.items),
+        shipping_address=order.shipping_address,
+    )
+
+    # Validate products and calculate total
+    total_amount = 0.0
+    order_items = []
+
+    for item in order.items:
+        if item.product_id not in products_db:
+            log.error(
+                "Invalid product in order",
+                order_id=order_id,
+                product_id=item.product_id,
+                available_products=list(products_db.keys()),
+            )
+            raise HTTPException(
+                status_code=400, detail=f"Product {item.product_id} not found"
+            )
+
+        product = products_db[item.product_id]
+
+        if not product["in_stock"]:
+            log.warning(
+                "Out of stock product in order",
+                order_id=order_id,
+                product_id=item.product_id,
+                product_name=product["name"],
+                requested_quantity=item.quantity,
+            )
+            raise HTTPException(
+                status_code=400, detail=f"Product {product['name']} is out of stock"
+            )
+
+        item_total = product["price"] * item.quantity
+        total_amount += item_total
+
+        order_items.append(
+            {
+                "product_id": item.product_id,
+                "product_name": product["name"],
+                "quantity": item.quantity,
+                "unit_price": product["price"],
+                "item_total": item_total,
+            }
+        )
+
+    # Simulate order processing
+    await asyncio.sleep(0.2)
+
+    # Create order response
+    order_response = OrderResponse(
+        order_id=order_id,
+        customer_id=order.customer_id,
+        total_amount=total_amount,
+        status="confirmed",
+        created_at=datetime.now(),
+    )
+
+    # Store order
+    orders_db[order_id] = {
+        "order": order.dict(),
+        "response": order_response.dict(),
+        "items": order_items,
+    }
+
+    # Log business metrics
+    log.info(
+        "Order created successfully",
+        endpoint="/orders",
+        method="POST",
+        order_id=order_id,
+        customer_id=order.customer_id,
+        total_amount=total_amount,
+        item_count=len(order.items),
+        business_metrics={
+            "order_value": total_amount,
+            "items_ordered": len(order.items),
+            "unique_products": len(set(item.product_id for item in order.items)),
+        },
+    )
+
+    return order_response
+
+
+@app.get("/orders/{order_id}")
+async def get_order(order_id: str, timing: float = Depends(log_request_timing)):
+    """Get order details with audit logging."""
+    log.info(
+        "Order details requested",
+        endpoint=f"/orders/{order_id}",
+        method="GET",
+        order_id=order_id,
+    )
+
+    # Simulate database lookup
+    await asyncio.sleep(0.05)
+
+    if order_id not in orders_db:
+        log.warning(
+            "Order not found",
+            endpoint=f"/orders/{order_id}",
+            method="GET",
+            order_id=order_id,
+            total_orders=len(orders_db),
+        )
+        raise HTTPException(status_code=404, detail="Order not found")
+
+    order_data = orders_db[order_id]
+
+    log.info(
+        "Order retrieved successfully",
+        endpoint=f"/orders/{order_id}",
+        method="GET",
+        order_id=order_id,
+        customer_id=order_data["response"]["customer_id"],
+        total_amount=order_data["response"]["total_amount"],
+        status=order_data["response"]["status"],
+        created_at=order_data["response"]["created_at"],
+    )
+
+    return order_data["response"]
+
+
+@app.get("/analytics/orders")
+async def get_order_analytics():
+    """Get order analytics with business intelligence logging."""
+    log.info("Order analytics requested", endpoint="/analytics/orders", method="GET")
+
+    # Calculate analytics
+    total_orders = len(orders_db)
+    total_revenue = sum(
+        order["response"]["total_amount"] for order in orders_db.values()
+    )
+    avg_order_value = total_revenue / total_orders if total_orders > 0 else 0
+
+    # Product popularity
+    product_counts = {}
+    for order in orders_db.values():
+        for item in order["items"]:
+            product_id = item["product_id"]
+            product_counts[product_id] = (
+                product_counts.get(product_id, 0) + item["quantity"]
+            )
+
+    most_popular_product = (
+        max(product_counts.items(), key=lambda x: x[1]) if product_counts else None
+    )
+
+    # Log business intelligence
+    log.info(
+        "Order analytics calculated",
+        endpoint="/analytics/orders",
+        method="GET",
+        analytics={
+            "total_orders": total_orders,
+            "total_revenue": total_revenue,
+            "avg_order_value": avg_order_value,
+            "most_popular_product_id": (
+                most_popular_product[0] if most_popular_product else None
+            ),
+            "most_popular_product_quantity": (
+                most_popular_product[1] if most_popular_product else 0
+            ),
+        },
+        business_insights={
+            "revenue_per_order": avg_order_value,
+            "total_products_ordered": sum(product_counts.values()),
+            "unique_products_ordered": len(product_counts),
+        },
+    )
+
+    return {
+        "total_orders": total_orders,
+        "total_revenue": total_revenue,
+        "avg_order_value": avg_order_value,
+        "most_popular_product": most_popular_product,
+    }
+
+
+@app.get("/performance/slow")
+async def slow_endpoint():
+    """Slow endpoint to demonstrate performance monitoring."""
+    log.info("Slow endpoint started", endpoint="/performance/slow", method="GET")
+
+    # Simulate slow operation
+    await asyncio.sleep(3)
+
+    log.info(
+        "Slow endpoint completed",
+        endpoint="/performance/slow",
+        method="GET",
+        duration_seconds=3,
+    )
+
+    return {"message": "Slow operation completed"}
+
+
+@app.get("/performance/fast")
+async def fast_endpoint():
+    """Fast endpoint for comparison."""
+    log.info("Fast endpoint accessed", endpoint="/performance/fast", method="GET")
+
+    return {"message": "Fast operation completed"}
+
+
+@app.get("/metrics/business")
+async def business_metrics():
+    """Business metrics endpoint with comprehensive logging."""
+    log.info("Business metrics requested", endpoint="/metrics/business", method="GET")
+
+    # Calculate various business metrics
+    total_products = len(products_db)
+    in_stock_products = sum(1 for p in products_db.values() if p["in_stock"])
+    total_orders = len(orders_db)
+    total_revenue = sum(
+        order["response"]["total_amount"] for order in orders_db.values()
+    )
+
+    # Category breakdown
+    category_revenue = {}
+    for order in orders_db.values():
+        for item in order["items"]:
+            product = products_db[item["product_id"]]
+            category = product["category"]
+            revenue = item["item_total"]
+            category_revenue[category] = category_revenue.get(category, 0) + revenue
+
+    log.info(
+        "Business metrics calculated",
+        endpoint="/metrics/business",
+        method="GET",
+        metrics={
+            "total_products": total_products,
+            "in_stock_products": in_stock_products,
+            "stock_ratio": (
+                in_stock_products / total_products if total_products > 0 else 0
+            ),
+            "total_orders": total_orders,
+            "total_revenue": total_revenue,
+            "avg_order_value": total_revenue / total_orders if total_orders > 0 else 0,
+            "category_revenue": category_revenue,
+        },
+    )
+
+    return {
+        "total_products": total_products,
+        "in_stock_products": in_stock_products,
+        "stock_ratio": in_stock_products / total_products if total_products > 0 else 0,
+        "total_orders": total_orders,
+        "total_revenue": total_revenue,
+        "avg_order_value": total_revenue / total_orders if total_orders > 0 else 0,
+        "category_revenue": category_revenue,
+    }
+
+
+# Middleware for response logging
+@app.middleware("http")
+async def response_logging_middleware(request: Request, call_next):
+    """Middleware to log response timing and metadata."""
+    start_time = time.perf_counter()
+
+    # Process the request
+    response = await call_next(request)
+
+    # Calculate timing
+    duration = time.perf_counter() - start_time
+    duration_ms = round(duration * 1000, 2)
+
+    # Log response metrics
+    log.info(
+        "Response metrics",
+        endpoint=str(request.url.path),
+        method=request.method,
+        status_code=response.status_code,
+        duration_ms=duration_ms,
+        response_size=len(str(response.body)) if hasattr(response, "body") else 0,
+        performance_category=(
+            "slow" if duration_ms > 1000 else "fast" if duration_ms < 100 else "normal"
+        ),
+    )
+
+    return response
+
+
+def main():
+    """Run the FastAPI application."""
+    import uvicorn
+
+    print("=== FastAPI Structured Logging Example ===")
+    print("This example demonstrates:")
+    print("‚úÖ Performance monitoring and timing")
+    print("‚úÖ Business metrics and analytics")
+    print("‚úÖ Comprehensive request/response logging")
+    print("‚úÖ Audit trails and tracking")
+    print("‚úÖ Business intelligence logging")
+    print("‚úÖ Structured data in all logs")
+    print()
+    print("Available endpoints:")
+    print("  GET  /products              - List products with filtering")
+    print("  GET  /products/{id}         - Get product details")
+    print("  POST /orders                - Create order (comprehensive logging)")
+    print("  GET  /orders/{id}           - Get order details")
+    print("  GET  /analytics/orders      - Order analytics")
+    print("  GET  /performance/slow      - Slow endpoint for timing")
+    print("  GET  /performance/fast      - Fast endpoint for comparison")
+    print("  GET  /metrics/business      - Business metrics")
+    print()
+    print("Key logging features:")
+    print("‚úÖ Request timing and performance metrics")
+    print("‚úÖ Business metrics and KPIs")
+    print("‚úÖ Error tracking and debugging")
+    print("‚úÖ Audit trails for compliance")
+    print("‚úÖ Structured data for analysis")
+    print()
+    print(
+        "Start the server with: uvicorn examples.08_fastapi_structured_logging:app --reload"
+    )
+    print("Then visit: http://localhost:8000/docs")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/09_queue_configuration.py b/examples/09_queue_configuration.py
new file mode 100644
index 0000000..f053cf3
--- /dev/null
+++ b/examples/09_queue_configuration.py
@@ -0,0 +1,54 @@
+#!/usr/bin/env python3
+"""
+Example 9: Queue Configuration
+
+This example demonstrates how to configure and use the async logging queue in fapilog.
+It shows how queue settings affect logging behavior and performance.
+"""
+
+import time
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+def main():
+    print("=== Queue Configuration Example ===\n")
+
+    # Configure logging with queue enabled
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",
+        queue_enabled=True,  # Enable async queue
+        queue_maxsize=5,  # Small queue for demonstration
+        queue_batch_size=2,  # Process 2 events per batch
+        queue_batch_timeout=0.5,  # 0.5s max wait for batch
+        queue_overflow="drop",  # Drop logs if queue is full
+    )
+    configure_logging(settings=settings)
+
+    print("Queue settings:")
+    print(f"  queue_enabled: {settings.queue_enabled}")
+    print(f"  queue_maxsize: {settings.queue_maxsize}")
+    print(f"  queue_batch_size: {settings.queue_batch_size}")
+    print(f"  queue_batch_timeout: {settings.queue_batch_timeout}")
+    print(f"  queue_overflow: {settings.queue_overflow}")
+    print()
+
+    print("Logging 10 events rapidly to demonstrate queue behavior...")
+    for i in range(10):
+        log.info("Queue test event", event_number=i)
+        time.sleep(0.05)  # Rapid fire
+
+    print("\nAll events enqueued. Waiting for queue to flush...")
+    time.sleep(2)  # Wait for queue to process
+
+    print("\nKey takeaways:")
+    print("‚úÖ Async queue decouples logging from main thread")
+    print("‚úÖ Small queue size can drop logs if overwhelmed")
+    print("‚úÖ Batch size and timeout control flush behavior")
+    print("‚úÖ Overflow strategy determines what happens when full")
+    print("‚úÖ Use larger queue and batch size for production")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/10_overflow_strategies.py b/examples/10_overflow_strategies.py
new file mode 100644
index 0000000..bcdaac9
--- /dev/null
+++ b/examples/10_overflow_strategies.py
@@ -0,0 +1,59 @@
+#!/usr/bin/env python3
+"""
+Example 10: Queue Overflow Strategies
+
+This example demonstrates the different queue overflow strategies in fapilog:
+- drop: Silently drops logs when the queue is full
+- block: Waits for space in the queue (may slow down logging)
+- sample: Probabilistically keeps a fraction of logs when full
+"""
+
+import time
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+def log_events(strategy: str, sampling_rate: float = 1.0):
+    print(
+        f"\n--- Overflow Strategy: {strategy.upper()} (sampling_rate={sampling_rate}) ---"
+    )
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",
+        queue_enabled=True,
+        queue_maxsize=5,  # Small queue for demonstration
+        queue_batch_size=2,
+        queue_batch_timeout=0.5,
+        queue_overflow=strategy,
+        sampling_rate=sampling_rate,
+    )
+    configure_logging(settings=settings)
+
+    print(f"Logging 15 events rapidly with overflow strategy '{strategy}'...")
+    for i in range(15):
+        log.info("Overflow strategy test event", event_number=i)
+        time.sleep(0.02)
+    print("Waiting for queue to flush...")
+    time.sleep(2)
+
+
+def main():
+    print("=== Queue Overflow Strategies Example ===\n")
+    print("This example demonstrates:")
+    print("- drop: Drops logs when full (default)")
+    print("- block: Waits for space in the queue")
+    print("- sample: Keeps a fraction of logs when full (sampling_rate)")
+
+    log_events("drop")
+    log_events("block")
+    log_events("sample", sampling_rate=0.3)
+
+    print("\nKey takeaways:")
+    print("‚úÖ drop: Fastest, but may lose logs under load")
+    print("‚úÖ block: No log loss, but may slow down app")
+    print("‚úÖ sample: Adaptive, keeps a sample under load")
+    print("‚úÖ Choose based on your reliability/performance needs")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/11_performance_testing.py b/examples/11_performance_testing.py
new file mode 100644
index 0000000..2dfd3a1
--- /dev/null
+++ b/examples/11_performance_testing.py
@@ -0,0 +1,46 @@
+#!/usr/bin/env python3
+"""
+Example 11: Performance Testing
+
+This example demonstrates simple performance/load testing of the fapilog queue.
+It logs a large number of events as quickly as possible and measures throughput.
+"""
+
+import time
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+def main():
+    print("=== Performance Testing Example ===\n")
+    num_events = 1000
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="json",
+        queue_enabled=True,
+        queue_maxsize=500,
+        queue_batch_size=50,
+        queue_batch_timeout=0.5,
+        queue_overflow="drop",
+    )
+    configure_logging(settings=settings)
+
+    print(f"Logging {num_events} events as fast as possible...")
+    start = time.perf_counter()
+    for i in range(num_events):
+        log.info("Performance test event", event_number=i)
+    duration = time.perf_counter() - start
+    print(f"Enqueued {num_events} events in {duration:.3f} seconds")
+    print(f"Throughput: {num_events/duration:.1f} events/sec")
+
+    print("Waiting for queue to flush...")
+    time.sleep(2)
+    print("Done.")
+    print("\nKey takeaways:")
+    print("‚úÖ Queue can handle high throughput")
+    print("‚úÖ Batch size and queue size affect performance")
+    print("‚úÖ Use performance testing to tune your settings")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/12_batch_processing.py b/examples/12_batch_processing.py
new file mode 100644
index 0000000..a8c183b
--- /dev/null
+++ b/examples/12_batch_processing.py
@@ -0,0 +1,55 @@
+#!/usr/bin/env python3
+"""
+Example 12: Batch Processing Configuration
+
+This example demonstrates how batch size and batch timeout affect the logging queue's
+behavior in fapilog. It shows how logs are flushed in batches and how tuning these
+settings can optimize performance.
+"""
+
+import time
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+def log_with_batch(batch_size, batch_timeout, num_events=10):
+    print(f"\n--- Batch Size: {batch_size}, Batch Timeout: {batch_timeout}s ---")
+    settings = LoggingSettings(
+        level="INFO",
+        json_console="pretty",
+        queue_enabled=True,
+        queue_maxsize=20,
+        queue_batch_size=batch_size,
+        queue_batch_timeout=batch_timeout,
+        queue_overflow="drop",
+    )
+    configure_logging(settings=settings)
+
+    print(f"Logging {num_events} events...")
+    for i in range(num_events):
+        log.info("Batch processing test event", event_number=i)
+        time.sleep(0.1)
+    print("Waiting for queue to flush...")
+    time.sleep(2)
+
+
+def main():
+    print("=== Batch Processing Configuration Example ===\n")
+    print("This example demonstrates:")
+    print("- How batch size controls the number of events flushed at once")
+    print("- How batch timeout controls the maximum wait before flushing")
+    print("- How tuning these settings can optimize performance")
+
+    log_with_batch(batch_size=2, batch_timeout=1.0)
+    log_with_batch(batch_size=5, batch_timeout=0.5)
+    log_with_batch(batch_size=10, batch_timeout=0.2)
+
+    print("\nKey takeaways:")
+    print("‚úÖ Small batch size = more frequent flushes, lower latency")
+    print("‚úÖ Large batch size = higher throughput, but more delay")
+    print("‚úÖ Batch timeout ensures logs are not delayed indefinitely")
+    print("‚úÖ Tune for your workload and sink performance")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/examples/13_loki_sink.py b/examples/13_loki_sink.py
new file mode 100644
index 0000000..9e9ac76
--- /dev/null
+++ b/examples/13_loki_sink.py
@@ -0,0 +1,237 @@
+#!/usr/bin/env python3
+"""
+Example: Loki Sink Configuration
+
+This example demonstrates how to configure fapilog to send logs to Grafana Loki
+for centralized log aggregation and querying.
+
+Key features:
+- Environment variable configuration for sinks
+- Programmatic configuration via LoggingSettings
+- Multiple sink configuration patterns
+- Structured logging with metadata
+"""
+
+import asyncio
+import logging
+import time
+from typing import Dict, Any
+from contextlib import asynccontextmanager
+
+from fastapi import FastAPI, HTTPException
+from pydantic import BaseModel
+
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+class UserRequest(BaseModel):
+    user_id: str
+    action: str
+    data: Dict[str, Any]
+
+
+# Method 1: Environment Variable Configuration (Recommended)
+# Set these environment variables before running:
+# export FAPILOG_SINKS=stdout,loki
+# export FAPILOG_LEVEL=INFO
+# export FAPILOG_QUEUE_ENABLED=true
+
+
+def demonstrate_environment_config():
+    """Demonstrate configuration via environment variables."""
+    print("=== Environment Variable Configuration ===")
+    print("Set these environment variables:")
+    print("  export FAPILOG_SINKS=stdout,loki")
+    print("  export FAPILOG_LEVEL=INFO")
+    print("  export FAPILOG_QUEUE_ENABLED=true")
+    print("  export LOKI_URL=http://localhost:3100")
+    print()
+
+    # Configure logging using environment variables
+    logger = configure_logging()
+
+    logger.info("Application started with environment configuration")
+    logger.info(
+        "User action processed",
+        extra={"user_id": "user-123", "action": "login", "sinks": "stdout,loki"},
+    )
+
+
+# Method 2: Programmatic Configuration
+def demonstrate_programmatic_config():
+    """Demonstrate configuration via LoggingSettings."""
+    print("\n=== Programmatic Configuration ===")
+
+    # Configure via settings object
+    settings = LoggingSettings(
+        level="INFO",
+        sinks=["stdout", "loki"],  # List of sink names
+        queue_enabled=True,
+        queue_maxsize=1000,
+        queue_batch_size=10,
+        queue_overflow="drop",
+    )
+
+    logger = configure_logging(settings=settings)
+
+    logger.info("Application started with programmatic configuration")
+    logger.info(
+        "User action processed",
+        extra={
+            "user_id": "user-456",
+            "action": "logout",
+            "config_method": "programmatic",
+        },
+    )
+
+
+# Method 3: Mixed Configuration (Environment + Overrides)
+def demonstrate_mixed_config():
+    """Demonstrate mixing environment variables with programmatic overrides."""
+    print("\n=== Mixed Configuration ===")
+
+    # Start with environment settings, override specific values
+    settings = LoggingSettings(
+        level="DEBUG",  # Override environment level
+        sinks=["stdout"],  # Override to just stdout for development
+        queue_enabled=True,
+    )
+
+    logger = configure_logging(settings=settings)
+
+    logger.debug("Debug message with mixed configuration")
+    logger.info(
+        "User action processed",
+        extra={
+            "user_id": "user-789",
+            "action": "profile_update",
+            "config_method": "mixed",
+        },
+    )
+
+
+@asynccontextmanager
+async def lifespan(app: FastAPI):
+    """Application lifespan manager."""
+    log.info("Starting FastAPI application with Loki logging")
+    yield
+    log.info("Shutting down FastAPI application")
+
+
+app = FastAPI(lifespan=lifespan)
+
+
+@app.get("/")
+async def root():
+    """Root endpoint with structured logging."""
+    log.info(
+        "Root endpoint accessed",
+        extra={"endpoint": "/", "method": "GET", "user_agent": "example-client"},
+    )
+    return {"message": "Hello World"}
+
+
+@app.post("/users/{user_id}/action")
+async def user_action(user_id: str, request: UserRequest):
+    """User action endpoint with detailed logging."""
+    start_time = time.time()
+
+    # Log the incoming request
+    log.info(
+        "User action requested",
+        extra={
+            "user_id": user_id,
+            "action": request.action,
+            "data_size": len(str(request.data)),
+            "endpoint": f"/users/{user_id}/action",
+            "method": "POST",
+        },
+    )
+
+    try:
+        # Simulate some processing
+        await asyncio.sleep(0.1)
+
+        # Log successful processing
+        processing_time = time.time() - start_time
+        log.info(
+            "User action processed successfully",
+            extra={
+                "user_id": user_id,
+                "action": request.action,
+                "processing_time_ms": round(processing_time * 1000, 2),
+                "status": "success",
+            },
+        )
+
+        return {
+            "user_id": user_id,
+            "action": request.action,
+            "status": "success",
+            "processing_time_ms": round(processing_time * 1000, 2),
+        }
+
+    except Exception as e:
+        # Log error with context
+        log.error(
+            "User action failed",
+            extra={
+                "user_id": user_id,
+                "action": request.action,
+                "error": str(e),
+                "error_type": type(e).__name__,
+                "status": "error",
+            },
+        )
+        raise HTTPException(status_code=500, detail="Internal server error")
+
+
+@app.get("/health")
+async def health_check():
+    """Health check endpoint."""
+    log.info(
+        "Health check requested",
+        extra={"endpoint": "/health", "method": "GET", "timestamp": time.time()},
+    )
+    return {"status": "healthy"}
+
+
+@app.get("/metrics")
+async def get_metrics():
+    """Metrics endpoint with performance logging."""
+    log.info(
+        "Metrics requested",
+        extra={"endpoint": "/metrics", "method": "GET", "metrics_type": "application"},
+    )
+    return {"active_connections": 10, "memory_usage": "45MB", "uptime": "2h 30m"}
+
+
+if __name__ == "__main__":
+    import uvicorn
+
+    print("=== Loki Sink Configuration Examples ===\n")
+
+    # Demonstrate different configuration methods
+    demonstrate_environment_config()
+    demonstrate_programmatic_config()
+    demonstrate_mixed_config()
+
+    print("\n=== Starting FastAPI Server ===")
+    print("Configure sinks via environment variables:")
+    print("  FAPILOG_SINKS=stdout,loki")
+    print("  LOKI_URL=http://localhost:3100")
+    print()
+
+    # For the server, use environment-based configuration
+    log.info(
+        "Starting FastAPI server with Loki integration",
+        extra={"host": "0.0.0.0", "port": 8000, "loki_url": "http://localhost:3100"},
+    )
+
+    uvicorn.run(
+        app,
+        host="0.0.0.0",
+        port=8000,
+        log_config=None,  # Disable uvicorn's default logging
+    )
diff --git a/examples/14_multiple_sinks.py b/examples/14_multiple_sinks.py
new file mode 100644
index 0000000..e810098
--- /dev/null
+++ b/examples/14_multiple_sinks.py
@@ -0,0 +1,395 @@
+#!/usr/bin/env python3
+"""
+Example: Multiple Sinks Configuration
+
+This example demonstrates how to configure fapilog with multiple sinks
+for different output destinations simultaneously.
+
+Key features:
+- Multiple sinks configured via environment variables
+- Different sink combinations for different environments
+- Structured logging across all sinks
+- Error handling and sink-specific configurations
+"""
+
+import asyncio
+import logging
+import time
+import tempfile
+import os
+from typing import Dict, Any, List
+from contextlib import asynccontextmanager
+
+from fastapi import FastAPI, HTTPException, Request
+from pydantic import BaseModel
+
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+class OrderRequest(BaseModel):
+    customer_id: str
+    items: List[Dict[str, Any]]
+    total_amount: float
+
+
+class OrderResponse(BaseModel):
+    order_id: str
+    customer_id: str
+    status: str
+    total_amount: float
+    created_at: str
+
+
+# Create temporary log file for demonstration
+log_file = tempfile.NamedTemporaryFile(delete=False, suffix=".log")
+log_file_path = log_file.name
+log_file.close()
+
+
+def demonstrate_development_config():
+    """Demonstrate multiple sinks for development environment."""
+    print("=== Development Configuration ===")
+    print("Environment variables for development:")
+    print("  export FAPILOG_SINKS=stdout,file")
+    print("  export FAPILOG_LEVEL=DEBUG")
+    print("  export FAPILOG_JSON_CONSOLE=pretty")
+    print("  export FAPILOG_FILE_PATH=/tmp/app.log")
+    print()
+
+    # Configure for development (stdout + file)
+    settings = LoggingSettings(
+        level="DEBUG",
+        sinks=["stdout", "file"],
+        json_console="pretty",  # Pretty output for development
+        queue_enabled=True,
+        queue_maxsize=100,  # Smaller queue for development
+        queue_batch_size=5,
+    )
+
+    logger = configure_logging(settings=settings)
+
+    logger.info(
+        "Development environment configured",
+        extra={
+            "environment": "development",
+            "sinks": ["stdout", "file"],
+            "console_format": "pretty",
+        },
+    )
+
+
+def demonstrate_production_config():
+    """Demonstrate multiple sinks for production environment."""
+    print("\n=== Production Configuration ===")
+    print("Environment variables for production:")
+    print("  export FAPILOG_SINKS=stdout,loki")
+    print("  export FAPILOG_LEVEL=INFO")
+    print("  export FAPILOG_JSON_CONSOLE=json")
+    print("  export LOKI_URL=http://loki:3100")
+    print("  export LOKI_LABELS=app=ecommerce,env=prod")
+    print()
+
+    # Configure for production (stdout + loki)
+    settings = LoggingSettings(
+        level="INFO",
+        sinks=["stdout", "loki"],
+        json_console="json",  # JSON output for production
+        queue_enabled=True,
+        queue_maxsize=1000,  # Larger queue for production
+        queue_batch_size=20,
+        queue_overflow="drop",  # Drop logs if overwhelmed
+    )
+
+    logger = configure_logging(settings=settings)
+
+    logger.info(
+        "Production environment configured",
+        extra={
+            "environment": "production",
+            "sinks": ["stdout", "loki"],
+            "console_format": "json",
+        },
+    )
+
+
+def demonstrate_monitoring_config():
+    """Demonstrate multiple sinks for monitoring/observability."""
+    print("\n=== Monitoring Configuration ===")
+    print("Environment variables for monitoring:")
+    print("  export FAPILOG_SINKS=stdout,loki,file")
+    print("  export FAPILOG_LEVEL=WARNING")
+    print("  export FAPILOG_ENABLE_RESOURCE_METRICS=true")
+    print("  export FAPILOG_FILE_PATH=/var/log/app/errors.log")
+    print()
+
+    # Configure for monitoring (stdout + loki + file for errors)
+    settings = LoggingSettings(
+        level="WARNING",  # Only warnings and errors
+        sinks=["stdout", "loki", "file"],
+        enable_resource_metrics=True,  # Enable memory/CPU metrics
+        queue_enabled=True,
+        queue_maxsize=500,
+        queue_batch_size=10,
+        queue_overflow="sample",  # Sample when overwhelmed
+        sampling_rate=0.8,  # Keep 80% of logs
+    )
+
+    logger = configure_logging(settings=settings)
+
+    logger.warning(
+        "Monitoring environment configured",
+        extra={
+            "environment": "monitoring",
+            "sinks": ["stdout", "loki", "file"],
+            "resource_metrics": True,
+        },
+    )
+
+
+def demonstrate_environment_variables():
+    """Demonstrate configuration via environment variables only."""
+    print("\n=== Environment Variables Only ===")
+    print("Set these environment variables:")
+    print("  export FAPILOG_SINKS=stdout,file,loki")
+    print("  export FAPILOG_LEVEL=INFO")
+    print("  export FAPILOG_QUEUE_ENABLED=true")
+    print("  export FAPILOG_QUEUE_MAXSIZE=1000")
+    print("  export FAPILOG_QUEUE_BATCH_SIZE=10")
+    print("  export FAPILOG_QUEUE_OVERFLOW=drop")
+    print("  export FAPILOG_ENABLE_RESOURCE_METRICS=true")
+    print("  export LOKI_URL=http://localhost:3100")
+    print("  export FAPILOG_FILE_PATH=/tmp/app.log")
+    print()
+
+    # Configure using environment variables only
+    logger = configure_logging()
+
+    logger.info(
+        "Environment-based configuration active",
+        extra={"config_method": "environment_variables", "sinks": "stdout,file,loki"},
+    )
+
+
+@asynccontextmanager
+async def lifespan(app: FastAPI):
+    """Application lifespan manager."""
+    log.info(
+        "Starting e-commerce API with multiple logging sinks",
+        extra={"log_file": log_file_path, "sinks": ["stdout", "file", "loki"]},
+    )
+    yield
+    log.info("Shutting down e-commerce API")
+    # Clean up log file
+    try:
+        os.unlink(log_file_path)
+    except OSError:
+        pass
+
+
+app = FastAPI(lifespan=lifespan)
+
+
+@app.middleware("http")
+async def log_requests(request: Request, call_next):
+    """Middleware to log all requests."""
+    start_time = time.time()
+
+    # Log request details
+    log.debug(
+        "Incoming request",
+        extra={
+            "method": request.method,
+            "url": str(request.url),
+            "client_ip": request.client.host if request.client else "unknown",
+            "user_agent": request.headers.get("user-agent", "unknown"),
+        },
+    )
+
+    try:
+        response = await call_next(request)
+
+        # Log response details
+        processing_time = time.time() - start_time
+        log.info(
+            "Request processed",
+            extra={
+                "method": request.method,
+                "url": str(request.url),
+                "status_code": response.status_code,
+                "processing_time_ms": round(processing_time * 1000, 2),
+            },
+        )
+
+        return response
+
+    except Exception as e:
+        # Log errors
+        processing_time = time.time() - start_time
+        log.error(
+            "Request failed",
+            extra={
+                "method": request.method,
+                "url": str(request.url),
+                "error": str(e),
+                "processing_time_ms": round(processing_time * 1000, 2),
+            },
+        )
+        raise
+
+
+@app.get("/")
+async def root():
+    """Root endpoint."""
+    log.info("Root endpoint accessed")
+    return {"message": "E-commerce API", "version": "1.0.0"}
+
+
+@app.post("/orders", response_model=OrderResponse)
+async def create_order(request: OrderRequest):
+    """Create a new order with comprehensive logging."""
+    order_id = f"ORD-{int(time.time())}"
+
+    # Log order creation start
+    log.info(
+        "Order creation started",
+        extra={
+            "order_id": order_id,
+            "customer_id": request.customer_id,
+            "item_count": len(request.items),
+            "total_amount": request.total_amount,
+        },
+    )
+
+    try:
+        # Simulate order processing
+        await asyncio.sleep(0.2)
+
+        # Validate order
+        if request.total_amount <= 0:
+            log.warning(
+                "Invalid order amount",
+                extra={
+                    "order_id": order_id,
+                    "customer_id": request.customer_id,
+                    "total_amount": request.total_amount,
+                },
+            )
+            raise HTTPException(status_code=400, detail="Invalid order amount")
+
+        if len(request.items) == 0:
+            log.warning(
+                "Empty order",
+                extra={"order_id": order_id, "customer_id": request.customer_id},
+            )
+            raise HTTPException(status_code=400, detail="Order cannot be empty")
+
+        # Log successful order creation
+        log.info(
+            "Order created successfully",
+            extra={
+                "order_id": order_id,
+                "customer_id": request.customer_id,
+                "status": "created",
+                "total_amount": request.total_amount,
+            },
+        )
+
+        return OrderResponse(
+            order_id=order_id,
+            customer_id=request.customer_id,
+            status="created",
+            total_amount=request.total_amount,
+            created_at=time.strftime("%Y-%m-%d %H:%M:%S"),
+        )
+
+    except HTTPException:
+        # Re-raise HTTP exceptions
+        raise
+    except Exception as e:
+        # Log unexpected errors
+        log.error(
+            "Order creation failed",
+            extra={
+                "order_id": order_id,
+                "customer_id": request.customer_id,
+                "error": str(e),
+                "error_type": type(e).__name__,
+            },
+        )
+        raise HTTPException(status_code=500, detail="Internal server error")
+
+
+@app.get("/orders/{order_id}")
+async def get_order(order_id: str):
+    """Get order details."""
+    log.debug("Order lookup requested", extra={"order_id": order_id})
+
+    # Simulate order lookup
+    await asyncio.sleep(0.1)
+
+    # Simulate order not found
+    if not order_id.startswith("ORD-"):
+        log.warning("Invalid order ID format", extra={"order_id": order_id})
+        raise HTTPException(status_code=400, detail="Invalid order ID format")
+
+    # Simulate order not found
+    if int(order_id.split("-")[1]) < time.time() - 3600:  # Order older than 1 hour
+        log.warning("Order not found", extra={"order_id": order_id})
+        raise HTTPException(status_code=404, detail="Order not found")
+
+    log.info("Order retrieved successfully", extra={"order_id": order_id})
+
+    return {
+        "order_id": order_id,
+        "customer_id": "CUST-123",
+        "status": "completed",
+        "total_amount": 99.99,
+        "created_at": "2024-01-15 10:30:00",
+    }
+
+
+@app.get("/health")
+async def health_check():
+    """Health check endpoint."""
+    log.debug("Health check requested")
+    return {"status": "healthy", "timestamp": time.time()}
+
+
+@app.get("/logs/file")
+async def get_log_file():
+    """Get the contents of the log file (for demonstration)."""
+    try:
+        with open(log_file_path, "r") as f:
+            logs = f.read()
+        return {"log_file": log_file_path, "contents": logs}
+    except FileNotFoundError:
+        return {"error": "Log file not found"}
+
+
+if __name__ == "__main__":
+    import uvicorn
+
+    print("=== Multiple Sinks Configuration Examples ===\n")
+
+    # Demonstrate different sink configurations
+    demonstrate_development_config()
+    demonstrate_production_config()
+    demonstrate_monitoring_config()
+    demonstrate_environment_variables()
+
+    print("\n=== Starting E-commerce API Server ===")
+    print("Configure multiple sinks via environment variables:")
+    print("  FAPILOG_SINKS=stdout,file,loki")
+    print("  FAPILOG_LEVEL=INFO")
+    print("  FAPILOG_QUEUE_ENABLED=true")
+    print("  LOKI_URL=http://localhost:3100")
+    print("  FAPILOG_FILE_PATH=/tmp/app.log")
+    print()
+
+    log.info(
+        "Starting e-commerce API server",
+        extra={"host": "0.0.0.0", "port": 8000, "log_file": log_file_path},
+    )
+
+    uvicorn.run(app, host="0.0.0.0", port=8000, log_config=None)
diff --git a/examples/15_custom_sink.py b/examples/15_custom_sink.py
new file mode 100644
index 0000000..f2c98bd
--- /dev/null
+++ b/examples/15_custom_sink.py
@@ -0,0 +1,455 @@
+#!/usr/bin/env python3
+"""
+Example: Custom Sink Implementation
+
+This example demonstrates how to create a custom sink for specialized
+logging requirements, such as sending logs to a custom API or database.
+
+Key features:
+- Custom sink implementation following fapilog sink interface
+- Integration with fapilog's async queue system
+- Error handling and retry logic
+- Structured data formatting
+- Multiple ways to register and use custom sinks
+"""
+
+import asyncio
+import json
+import logging
+import time
+import aiohttp
+from typing import Dict, Any, Optional, List
+from contextlib import asynccontextmanager
+
+from fastapi import FastAPI, HTTPException
+from pydantic import BaseModel
+
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+from fapilog._internal.queue import Sink
+
+
+class UserAction(BaseModel):
+    user_id: str
+    action: str
+    resource: str
+    details: Dict[str, Any]
+
+
+class CustomAPISink(Sink):
+    """
+    Custom sink that sends logs to a custom API endpoint.
+
+    This sink demonstrates how to create a custom sink for specialized
+    logging requirements, such as sending logs to a custom monitoring
+    service or database.
+    """
+
+    def __init__(
+        self,
+        api_url: str,
+        api_key: Optional[str] = None,
+        batch_size: int = 10,
+        batch_timeout: float = 5.0,
+        retry_attempts: int = 3,
+        retry_delay: float = 1.0,
+        timeout: float = 10.0,
+    ):
+        self.api_url = api_url
+        self.api_key = api_key
+        self.batch_size = batch_size
+        self.batch_timeout = batch_timeout
+        self.retry_attempts = retry_attempts
+        self.retry_delay = retry_delay
+        self.timeout = timeout
+        self.session: Optional[aiohttp.ClientSession] = None
+        self.log_buffer: List[Dict[str, Any]] = []
+        self.last_send_time = time.time()
+        self._send_task: Optional[asyncio.Task] = None
+
+    async def start(self):
+        """Initialize the custom sink."""
+        self.session = aiohttp.ClientSession(
+            timeout=aiohttp.ClientTimeout(total=self.timeout)
+        )
+        self._send_task = asyncio.create_task(self._batch_sender())
+        logging.getLogger(__name__).info(f"Custom API sink started: {self.api_url}")
+
+    async def stop(self):
+        """Clean up the custom sink."""
+        if self._send_task:
+            self._send_task.cancel()
+            try:
+                await self._send_task
+            except asyncio.CancelledError:
+                pass
+
+        # Send any remaining logs
+        if self.log_buffer:
+            await self._send_logs(self.log_buffer)
+            self.log_buffer.clear()
+
+        if self.session:
+            await self.session.close()
+
+        logging.getLogger(__name__).info("Custom API sink stopped")
+
+    async def write(self, record: Dict[str, Any]):
+        """Write a log record to the custom API."""
+        # Add timestamp if not present
+        if "timestamp" not in record:
+            record["timestamp"] = time.time()
+
+        # Add sink metadata
+        record["sink"] = "custom_api"
+        record["api_url"] = self.api_url
+
+        self.log_buffer.append(record)
+
+        # Send immediately if buffer is full
+        if len(self.log_buffer) >= self.batch_size:
+            await self._send_logs(self.log_buffer)
+            self.log_buffer.clear()
+            self.last_send_time = time.time()
+
+    async def _batch_sender(self):
+        """Background task to send logs in batches."""
+        while True:
+            try:
+                await asyncio.sleep(self.batch_timeout)
+
+                # Check if we should send logs
+                if (
+                    self.log_buffer
+                    and time.time() - self.last_send_time >= self.batch_timeout
+                ):
+                    await self._send_logs(self.log_buffer)
+                    self.log_buffer.clear()
+                    self.last_send_time = time.time()
+
+            except asyncio.CancelledError:
+                break
+            except Exception as e:
+                logging.getLogger(__name__).error(f"Batch sender error: {e}")
+
+    async def _send_logs(self, logs: List[Dict[str, Any]]):
+        """Send logs to the custom API with retry logic."""
+        if not logs:
+            return
+
+        for attempt in range(self.retry_attempts):
+            try:
+                headers = {"Content-Type": "application/json"}
+                if self.api_key:
+                    headers["Authorization"] = f"Bearer {self.api_key}"
+
+                payload = {
+                    "logs": logs,
+                    "batch_size": len(logs),
+                    "timestamp": time.time(),
+                }
+
+                async with self.session.post(
+                    self.api_url, json=payload, headers=headers
+                ) as response:
+                    if response.status == 200:
+                        logging.getLogger(__name__).debug(
+                            f"Successfully sent {len(logs)} logs to custom API"
+                        )
+                        return
+                    else:
+                        error_text = await response.text()
+                        raise Exception(f"API returned {response.status}: {error_text}")
+
+            except Exception as e:
+                if attempt == self.retry_attempts - 1:
+                    logging.getLogger(__name__).error(
+                        f"Failed to send logs to custom API after {self.retry_attempts} attempts: {e}"
+                    )
+                else:
+                    logging.getLogger(__name__).warning(
+                        f"Failed to send logs to custom API (attempt {attempt + 1}): {e}"
+                    )
+                    await asyncio.sleep(self.retry_delay * (attempt + 1))
+
+
+class AuditLogSink(Sink):
+    """
+    Custom sink for audit logging to a separate storage.
+
+    This sink demonstrates how to create a specialized sink for
+    audit logging with different formatting and storage requirements.
+    """
+
+    def __init__(self, audit_file_path: str):
+        self.audit_file_path = audit_file_path
+        self.audit_buffer: List[Dict[str, Any]] = []
+        self._audit_task: Optional[asyncio.Task] = None
+
+    async def start(self):
+        """Initialize the audit sink."""
+        self._audit_task = asyncio.create_task(self._audit_writer())
+        logging.getLogger(__name__).info(f"Audit sink started: {self.audit_file_path}")
+
+    async def stop(self):
+        """Clean up the audit sink."""
+        if self._audit_task:
+            self._audit_task.cancel()
+            try:
+                await self._audit_task
+            except asyncio.CancelledError:
+                pass
+
+        # Write any remaining audit logs
+        if self.audit_buffer:
+            await self._write_audit_logs(self.audit_buffer)
+            self.audit_buffer.clear()
+
+        logging.getLogger(__name__).info("Audit sink stopped")
+
+    async def write(self, record: Dict[str, Any]):
+        """Write an audit log record."""
+        # Only process audit-related logs
+        if record.get("log_type") != "audit":
+            return
+
+        # Add audit-specific metadata
+        record["audit_timestamp"] = time.time()
+        record["audit_id"] = f"AUDIT-{int(time.time())}"
+
+        self.audit_buffer.append(record)
+
+        # Write immediately for audit logs (no batching for security)
+        if self.audit_buffer:
+            await self._write_audit_logs(self.audit_buffer)
+            self.audit_buffer.clear()
+
+    async def _audit_writer(self):
+        """Background task to write audit logs."""
+        while True:
+            try:
+                await asyncio.sleep(1.0)  # Check every second
+
+                if self.audit_buffer:
+                    await self._write_audit_logs(self.audit_buffer)
+                    self.audit_buffer.clear()
+
+            except asyncio.CancelledError:
+                break
+            except Exception as e:
+                logging.getLogger(__name__).error(f"Audit writer error: {e}")
+
+    async def _write_audit_logs(self, logs: List[Dict[str, Any]]):
+        """Write audit logs to file."""
+        try:
+            # Use asyncio to write to file
+            loop = asyncio.get_event_loop()
+            await loop.run_in_executor(None, self._write_logs_sync, logs)
+        except Exception as e:
+            logging.getLogger(__name__).error(f"Failed to write audit logs: {e}")
+
+    def _write_logs_sync(self, logs: List[Dict[str, Any]]):
+        """Synchronous file writing."""
+        try:
+            with open(self.audit_file_path, "a") as f:
+                for log in logs:
+                    f.write(json.dumps(log) + "\n")
+        except Exception as e:
+            logging.getLogger(__name__).error(f"Sync audit log write failed: {e}")
+
+
+def demonstrate_custom_sink_usage():
+    """Demonstrate how to use custom sinks with fapilog."""
+    print("=== Custom Sink Usage ===")
+    print("This example shows how to create and use custom sinks.")
+    print(
+        "Custom sinks should implement the Sink interface from fapilog._internal.queue"
+    )
+    print()
+
+    # Note: In a real application, you would register custom sinks
+    # with fapilog's sink registry or configure them through
+    # environment variables or settings.
+
+    print("Custom sink features:")
+    print("‚úÖ Implement async write() method")
+    print("‚úÖ Handle batching and retries")
+    print("‚úÖ Proper error handling and logging")
+    print("‚úÖ Graceful shutdown")
+    print("‚úÖ Integration with fapilog's queue system")
+    print()
+
+
+def demonstrate_audit_logging():
+    """Demonstrate audit logging patterns."""
+    print("\n=== Audit Logging Pattern ===")
+
+    # Configure logging with standard sinks
+    settings = LoggingSettings(
+        level="INFO", sinks=["stdout"], queue_enabled=True  # Standard sinks
+    )
+
+    logger = configure_logging(settings=settings)
+
+    # Log regular application events
+    logger.info(
+        "User action received",
+        extra={"user_id": "user-123", "action": "login", "resource": "auth"},
+    )
+
+    # Log audit events (these would be handled by custom audit sink)
+    logger.info(
+        "User action audit",
+        extra={
+            "log_type": "audit",  # Special marker for audit sink
+            "user_id": "user-123",
+            "action": "login",
+            "resource": "auth",
+            "details": {"ip": "192.168.1.100", "session_id": "session-123"},
+            "ip_address": "192.168.1.100",
+            "session_id": "session-123",
+        },
+    )
+
+    print("Audit logging features:")
+    print("‚úÖ Separate storage for audit logs")
+    print("‚úÖ Immediate writing (no batching)")
+    print("‚úÖ Enhanced metadata and security")
+    print("‚úÖ Compliance-friendly formatting")
+    print()
+
+
+def demonstrate_custom_api_integration():
+    """Demonstrate custom API integration."""
+    print("\n=== Custom API Integration ===")
+
+    # Configure logging
+    settings = LoggingSettings(
+        level="INFO",
+        sinks=["stdout"],
+        queue_enabled=True,
+        queue_batch_size=5,  # Smaller batches for API calls
+    )
+
+    logger = configure_logging(settings=settings)
+
+    # Log events that would be sent to custom API
+    logger.info(
+        "Application event",
+        extra={
+            "event_type": "user_action",
+            "user_id": "user-456",
+            "action": "profile_update",
+            "api_destination": "monitoring-service",
+        },
+    )
+
+    print("Custom API integration features:")
+    print("‚úÖ Batch sending for efficiency")
+    print("‚úÖ Retry logic with exponential backoff")
+    print("‚úÖ Authentication and headers")
+    print("‚úÖ Error handling and monitoring")
+    print("‚úÖ Structured payload formatting")
+    print()
+
+
+@asynccontextmanager
+async def lifespan(app: FastAPI):
+    """Application lifespan manager."""
+    log.info("Starting application with custom sinks")
+    yield
+    log.info("Shutting down application")
+
+
+app = FastAPI(lifespan=lifespan)
+
+
+@app.post("/users/{user_id}/action")
+async def user_action(user_id: str, action: UserAction):
+    """User action endpoint with custom logging."""
+
+    # Regular application log
+    log.info(
+        "User action received",
+        extra={
+            "user_id": user_id,
+            "action": action.action,
+            "resource": action.resource,
+        },
+    )
+
+    # Audit log for security (would be handled by custom audit sink)
+    log.info(
+        "User action audit",
+        extra={
+            "log_type": "audit",
+            "user_id": user_id,
+            "action": action.action,
+            "resource": action.resource,
+            "details": action.details,
+            "ip_address": "192.168.1.100",  # Mock IP
+            "session_id": "session-123",
+        },
+    )
+
+    # Simulate processing
+    await asyncio.sleep(0.1)
+
+    # Log success
+    log.info(
+        "User action completed",
+        extra={"user_id": user_id, "action": action.action, "status": "success"},
+    )
+
+    return {
+        "user_id": user_id,
+        "action": action.action,
+        "status": "completed",
+        "timestamp": time.time(),
+    }
+
+
+@app.get("/health")
+async def health_check():
+    """Health check endpoint."""
+    log.info("Health check requested")
+    return {"status": "healthy"}
+
+
+@app.get("/audit/logs")
+async def get_audit_logs():
+    """Get audit logs (for demonstration)."""
+    try:
+        with open("/tmp/audit.log", "r") as f:
+            logs = f.readlines()
+        return {"audit_logs": [json.loads(log.strip()) for log in logs]}
+    except FileNotFoundError:
+        return {"audit_logs": []}
+
+
+if __name__ == "__main__":
+    import uvicorn
+
+    print("=== Custom Sink Implementation Examples ===\n")
+
+    # Demonstrate different custom sink patterns
+    demonstrate_custom_sink_usage()
+    demonstrate_audit_logging()
+    demonstrate_custom_api_integration()
+
+    print("\n=== Starting Application Server ===")
+    print("Custom sinks would be configured via:")
+    print("  - Environment variables")
+    print("  - Sink registry")
+    print("  - Programmatic configuration")
+    print()
+
+    log.info(
+        "Starting application with custom sinks",
+        extra={
+            "custom_api_url": "http://localhost:8080/api/logs",
+            "audit_file": "/tmp/audit.log",
+        },
+    )
+
+    uvicorn.run(app, host="0.0.0.0", port=8000, log_config=None)
diff --git a/examples/16_security_logging.py b/examples/16_security_logging.py
new file mode 100644
index 0000000..b3d43ea
--- /dev/null
+++ b/examples/16_security_logging.py
@@ -0,0 +1,505 @@
+#!/usr/bin/env python3
+"""
+Example: Security and Compliance Logging
+
+This example demonstrates security-focused logging patterns including
+audit trails, sensitive data handling, and compliance logging.
+
+Key features:
+- Audit trail logging for security events
+- Sensitive data masking and filtering
+- Compliance logging (GDPR, SOX, etc.)
+- Security event correlation
+- Access control logging
+"""
+
+import asyncio
+import hashlib
+import json
+import logging
+import time
+import uuid
+from datetime import datetime, timezone
+from typing import Dict, Any, Optional, List
+from contextlib import asynccontextmanager
+
+from fastapi import FastAPI, HTTPException, Request, Depends, HTTPException
+from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
+from pydantic import BaseModel, Field
+
+from fapilog import bootstrap_logger
+from fapilog.settings import LogSettings
+from fapilog.sinks.stdout import StdoutSink
+
+
+class User(BaseModel):
+    user_id: str
+    email: str
+    role: str
+    permissions: List[str]
+
+
+class LoginRequest(BaseModel):
+    email: str
+    password: str
+
+
+class SensitiveData(BaseModel):
+    credit_card: str = Field(..., description="Credit card number")
+    ssn: str = Field(..., description="Social Security Number")
+    address: str = Field(..., description="Home address")
+
+
+class SecurityEvent(BaseModel):
+    event_type: str
+    user_id: Optional[str] = None
+    ip_address: str
+    user_agent: str
+    details: Dict[str, Any]
+
+
+# Security utilities
+def mask_sensitive_data(data: str, mask_char: str = "*") -> str:
+    """Mask sensitive data for logging."""
+    if len(data) <= 4:
+        return mask_char * len(data)
+    return data[:2] + mask_char * (len(data) - 4) + data[-2:]
+
+
+def hash_sensitive_data(data: str) -> str:
+    """Hash sensitive data for secure logging."""
+    return hashlib.sha256(data.encode()).hexdigest()[:16]
+
+
+def sanitize_log_data(data: Dict[str, Any]) -> Dict[str, Any]:
+    """Sanitize log data by masking sensitive fields."""
+    sensitive_fields = {
+        "password",
+        "credit_card",
+        "ssn",
+        "token",
+        "secret",
+        "api_key",
+        "private_key",
+        "session_id",
+    }
+
+    sanitized = data.copy()
+    for key, value in sanitized.items():
+        if key.lower() in sensitive_fields and isinstance(value, str):
+            sanitized[key] = mask_sensitive_data(value)
+        elif isinstance(value, dict):
+            sanitized[key] = sanitize_log_data(value)
+        elif isinstance(value, list):
+            sanitized[key] = [
+                sanitize_log_data(item) if isinstance(item, dict) else item
+                for item in value
+            ]
+
+    return sanitized
+
+
+# Configure security-focused logging
+security_settings = LogSettings(
+    level="INFO",
+    format="json",
+    sinks=[
+        StdoutSink(
+            level="INFO",
+            format="json",
+        )
+    ],
+)
+
+# Initialize logger
+logger = bootstrap_logger(security_settings)
+
+# Security dependencies
+security = HTTPBearer()
+
+
+def get_current_user(
+    credentials: HTTPAuthorizationCredentials = Depends(security),
+) -> User:
+    """Get current user from token (simplified for demo)."""
+    # In real app, validate JWT token
+    token = credentials.credentials
+
+    # Simulate token validation
+    if token == "valid-token":
+        return User(
+            user_id="user-123",
+            email="user@example.com",
+            role="admin",
+            permissions=["read", "write", "delete"],
+        )
+    else:
+        raise HTTPException(status_code=401, detail="Invalid token")
+
+
+@asynccontextmanager
+async def lifespan(app: FastAPI):
+    """Application lifespan manager."""
+    logger.info(
+        "Starting secure application",
+        extra={
+            "security_level": "high",
+            "compliance": ["GDPR", "SOX", "PCI-DSS"],
+            "audit_enabled": True,
+        },
+    )
+    yield
+    logger.info("Shutting down secure application")
+
+
+app = FastAPI(lifespan=lifespan)
+
+
+@app.middleware("http")
+async def security_middleware(request: Request, call_next):
+    """Security middleware for logging all requests."""
+    start_time = time.time()
+    request_id = str(uuid.uuid4())
+
+    # Extract security-relevant information
+    client_ip = request.client.host if request.client else "unknown"
+    user_agent = request.headers.get("user-agent", "unknown")
+
+    # Log incoming request
+    logger.info(
+        "Incoming request",
+        extra={
+            "log_type": "security",
+            "event_type": "request_start",
+            "request_id": request_id,
+            "method": request.method,
+            "url": str(request.url),
+            "client_ip": client_ip,
+            "user_agent": user_agent,
+            "timestamp": datetime.now(timezone.utc).isoformat(),
+        },
+    )
+
+    try:
+        response = await call_next(request)
+
+        # Log successful request
+        processing_time = time.time() - start_time
+        logger.info(
+            "Request completed",
+            extra={
+                "log_type": "security",
+                "event_type": "request_success",
+                "request_id": request_id,
+                "status_code": response.status_code,
+                "processing_time_ms": round(processing_time * 1000, 2),
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
+        return response
+
+    except Exception as e:
+        # Log failed request
+        processing_time = time.time() - start_time
+        logger.error(
+            "Request failed",
+            extra={
+                "log_type": "security",
+                "event_type": "request_error",
+                "request_id": request_id,
+                "error": str(e),
+                "error_type": type(e).__name__,
+                "processing_time_ms": round(processing_time * 1000, 2),
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+        raise
+
+
+@app.post("/auth/login")
+async def login(request: LoginRequest):
+    """Login endpoint with security logging."""
+    # Log login attempt
+    logger.info(
+        "Login attempt",
+        extra={
+            "log_type": "security",
+            "event_type": "login_attempt",
+            "email": mask_sensitive_data(request.email),
+            "ip_address": "192.168.1.100",  # Mock IP
+            "user_agent": "Mozilla/5.0...",
+            "timestamp": datetime.now(timezone.utc).isoformat(),
+        },
+    )
+
+    # Simulate authentication
+    await asyncio.sleep(0.1)
+
+    if request.email == "user@example.com" and request.password == "password123":
+        # Log successful login
+        session_id = str(uuid.uuid4())
+        logger.info(
+            "Login successful",
+            extra={
+                "log_type": "security",
+                "event_type": "login_success",
+                "user_id": "user-123",
+                "email": mask_sensitive_data(request.email),
+                "session_id": session_id,
+                "ip_address": "192.168.1.100",
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
+        return {
+            "access_token": "valid-token",
+            "session_id": session_id,
+            "expires_in": 3600,
+        }
+    else:
+        # Log failed login
+        logger.warning(
+            "Login failed",
+            extra={
+                "log_type": "security",
+                "event_type": "login_failed",
+                "email": mask_sensitive_data(request.email),
+                "ip_address": "192.168.1.100",
+                "reason": "invalid_credentials",
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
+        raise HTTPException(status_code=401, detail="Invalid credentials")
+
+
+@app.post("/auth/logout")
+async def logout(current_user: User = Depends(get_current_user)):
+    """Logout endpoint with security logging."""
+    logger.info(
+        "Logout",
+        extra={
+            "log_type": "security",
+            "event_type": "logout",
+            "user_id": current_user.user_id,
+            "email": mask_sensitive_data(current_user.email),
+            "ip_address": "192.168.1.100",
+            "timestamp": datetime.now(timezone.utc).isoformat(),
+        },
+    )
+
+    return {"message": "Logged out successfully"}
+
+
+@app.post("/users/{user_id}/sensitive-data")
+async def process_sensitive_data(
+    user_id: str, data: SensitiveData, current_user: User = Depends(get_current_user)
+):
+    """Process sensitive data with secure logging."""
+
+    # Log data processing attempt
+    logger.info(
+        "Sensitive data processing",
+        extra={
+            "log_type": "security",
+            "event_type": "sensitive_data_access",
+            "user_id": current_user.user_id,
+            "target_user_id": user_id,
+            "data_type": "personal_information",
+            "ip_address": "192.168.1.100",
+            "timestamp": datetime.now(timezone.utc).isoformat(),
+        },
+    )
+
+    # Check permissions
+    if "write" not in current_user.permissions:
+        logger.warning(
+            "Permission denied",
+            extra={
+                "log_type": "security",
+                "event_type": "permission_denied",
+                "user_id": current_user.user_id,
+                "target_user_id": user_id,
+                "required_permission": "write",
+                "user_permissions": current_user.permissions,
+                "ip_address": "192.168.1.100",
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+        raise HTTPException(status_code=403, detail="Permission denied")
+
+    # Process data (simulated)
+    await asyncio.sleep(0.1)
+
+    # Log successful processing with sanitized data
+    sanitized_data = sanitize_log_data(data.dict())
+    logger.info(
+        "Sensitive data processed",
+        extra={
+            "log_type": "security",
+            "event_type": "sensitive_data_processed",
+            "user_id": current_user.user_id,
+            "target_user_id": user_id,
+            "data_hash": hash_sensitive_data(json.dumps(data.dict())),
+            "sanitized_data": sanitized_data,
+            "ip_address": "192.168.1.100",
+            "timestamp": datetime.now(timezone.utc).isoformat(),
+        },
+    )
+
+    return {
+        "user_id": user_id,
+        "status": "processed",
+        "data_hash": hash_sensitive_data(json.dumps(data.dict())),
+        "timestamp": datetime.now(timezone.utc).isoformat(),
+    }
+
+
+@app.get("/users/{user_id}/profile")
+async def get_user_profile(
+    user_id: str, current_user: User = Depends(get_current_user)
+):
+    """Get user profile with access logging."""
+
+    # Log access attempt
+    logger.info(
+        "Profile access",
+        extra={
+            "log_type": "security",
+            "event_type": "profile_access",
+            "user_id": current_user.user_id,
+            "target_user_id": user_id,
+            "ip_address": "192.168.1.100",
+            "timestamp": datetime.now(timezone.utc).isoformat(),
+        },
+    )
+
+    # Check if user can access this profile
+    if current_user.user_id != user_id and "admin" not in current_user.role:
+        logger.warning(
+            "Unauthorized profile access",
+            extra={
+                "log_type": "security",
+                "event_type": "unauthorized_access",
+                "user_id": current_user.user_id,
+                "target_user_id": user_id,
+                "ip_address": "192.168.1.100",
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+        raise HTTPException(status_code=403, detail="Access denied")
+
+    # Return profile data
+    return {
+        "user_id": user_id,
+        "email": mask_sensitive_data("user@example.com"),
+        "role": "user",
+        "created_at": "2024-01-01T00:00:00Z",
+    }
+
+
+@app.post("/security/events")
+async def report_security_event(event: SecurityEvent):
+    """Report security events for correlation."""
+
+    logger.warning(
+        "Security event reported",
+        extra={
+            "log_type": "security",
+            "event_type": event.event_type,
+            "user_id": event.user_id,
+            "ip_address": event.ip_address,
+            "user_agent": event.user_agent,
+            "details": sanitize_log_data(event.details),
+            "timestamp": datetime.now(timezone.utc).isoformat(),
+        },
+    )
+
+    return {"status": "event_logged", "event_id": str(uuid.uuid4())}
+
+
+@app.get("/audit/logs")
+async def get_audit_logs(
+    start_time: Optional[str] = None,
+    end_time: Optional[str] = None,
+    event_type: Optional[str] = None,
+    user_id: Optional[str] = None,
+    current_user: User = Depends(get_current_user),
+):
+    """Get audit logs (admin only)."""
+
+    if "admin" not in current_user.role:
+        logger.warning(
+            "Unauthorized audit log access",
+            extra={
+                "log_type": "security",
+                "event_type": "unauthorized_audit_access",
+                "user_id": current_user.user_id,
+                "ip_address": "192.168.1.100",
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+        raise HTTPException(status_code=403, detail="Admin access required")
+
+    # Log audit log access
+    logger.info(
+        "Audit logs accessed",
+        extra={
+            "log_type": "security",
+            "event_type": "audit_log_access",
+            "user_id": current_user.user_id,
+            "filters": {
+                "start_time": start_time,
+                "end_time": end_time,
+                "event_type": event_type,
+                "user_id": user_id,
+            },
+            "ip_address": "192.168.1.100",
+            "timestamp": datetime.now(timezone.utc).isoformat(),
+        },
+    )
+
+    # Return mock audit logs
+    return {
+        "audit_logs": [
+            {
+                "event_type": "login_success",
+                "user_id": "user-123",
+                "timestamp": "2024-01-15T10:30:00Z",
+                "ip_address": "192.168.1.100",
+            },
+            {
+                "event_type": "sensitive_data_access",
+                "user_id": "user-123",
+                "timestamp": "2024-01-15T10:35:00Z",
+                "ip_address": "192.168.1.100",
+            },
+        ],
+        "total_count": 2,
+        "filters_applied": {
+            "start_time": start_time,
+            "end_time": end_time,
+            "event_type": event_type,
+            "user_id": user_id,
+        },
+    }
+
+
+if __name__ == "__main__":
+    import uvicorn
+
+    logger.info(
+        "Starting secure application",
+        extra={
+            "security_features": [
+                "audit_logging",
+                "sensitive_data_masking",
+                "access_control",
+                "compliance_logging",
+            ],
+            "compliance_standards": ["GDPR", "SOX", "PCI-DSS"],
+        },
+    )
+
+    uvicorn.run(app, host="0.0.0.0", port=8000, log_config=None)
diff --git a/examples/test_corrected_examples.py b/examples/test_corrected_examples.py
new file mode 100644
index 0000000..97d1433
--- /dev/null
+++ b/examples/test_corrected_examples.py
@@ -0,0 +1,184 @@
+#!/usr/bin/env python3
+"""
+Test script for corrected sink examples.
+
+This script tests the corrected examples to ensure they work
+with the actual fapilog API.
+"""
+
+import sys
+from pathlib import Path
+
+# Add the src directory to the path
+sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
+
+from fapilog import configure_logging, log
+from fapilog.settings import LoggingSettings
+
+
+def test_environment_config():
+    """Test environment variable configuration."""
+    print("Testing environment variable configuration...")
+
+    try:
+        # Configure using environment variables (defaults)
+        logger = configure_logging()
+
+        logger.info(
+            "Environment configuration test",
+            extra={"test": True, "method": "environment"},
+        )
+
+        print("‚úÖ Environment configuration works")
+        return True
+
+    except Exception as e:
+        print(f"‚úó Environment configuration failed: {e}")
+        return False
+
+
+def test_programmatic_config():
+    """Test programmatic configuration."""
+    print("Testing programmatic configuration...")
+
+    try:
+        # Configure via settings object
+        settings = LoggingSettings(
+            level="INFO",
+            sinks=["stdout"],  # List of sink names
+            queue_enabled=True,
+            queue_maxsize=100,
+            queue_batch_size=5,
+        )
+
+        logger = configure_logging(settings=settings)
+
+        logger.info(
+            "Programmatic configuration test",
+            extra={"test": True, "method": "programmatic"},
+        )
+
+        print("‚úÖ Programmatic configuration works")
+        return True
+
+    except Exception as e:
+        print(f"‚úó Programmatic configuration failed: {e}")
+        return False
+
+
+def test_mixed_config():
+    """Test mixed configuration."""
+    print("Testing mixed configuration...")
+
+    try:
+        # Start with environment settings, override specific values
+        settings = LoggingSettings(
+            level="DEBUG",  # Override environment level
+            sinks=["stdout"],  # Override to just stdout
+            queue_enabled=True,
+        )
+
+        logger = configure_logging(settings=settings)
+
+        logger.debug("Debug message with mixed configuration")
+        logger.info("Mixed configuration test", extra={"test": True, "method": "mixed"})
+
+        print("‚úÖ Mixed configuration works")
+        return True
+
+    except Exception as e:
+        print(f"‚úó Mixed configuration failed: {e}")
+        return False
+
+
+def test_multiple_sinks_config():
+    """Test multiple sinks configuration."""
+    print("Testing multiple sinks configuration...")
+
+    try:
+        # Configure multiple sinks
+        settings = LoggingSettings(
+            level="INFO",
+            sinks=["stdout", "file"],  # Multiple sink names
+            queue_enabled=True,
+            queue_maxsize=100,
+            queue_batch_size=5,
+        )
+
+        logger = configure_logging(settings=settings)
+
+        logger.info(
+            "Multiple sinks test", extra={"test": True, "sinks": ["stdout", "file"]}
+        )
+
+        print("‚úÖ Multiple sinks configuration works")
+        return True
+
+    except Exception as e:
+        print(f"‚úó Multiple sinks configuration failed: {e}")
+        return False
+
+
+def test_logging_levels():
+    """Test different logging levels."""
+    print("Testing logging levels...")
+
+    try:
+        settings = LoggingSettings(level="DEBUG", sinks=["stdout"], queue_enabled=True)
+
+        logger = configure_logging(settings=settings)
+
+        logger.debug("Debug message")
+        logger.info("Info message")
+        logger.warning("Warning message")
+        logger.error("Error message")
+
+        print("‚úÖ Logging levels work")
+        return True
+
+    except Exception as e:
+        print(f"‚úó Logging levels failed: {e}")
+        return False
+
+
+def main():
+    """Run all tests."""
+    print("Testing corrected sink examples...\n")
+
+    tests = [
+        test_environment_config,
+        test_programmatic_config,
+        test_mixed_config,
+        test_multiple_sinks_config,
+        test_logging_levels,
+    ]
+
+    passed = 0
+    total = len(tests)
+
+    for test in tests:
+        try:
+            if test():
+                passed += 1
+        except Exception as e:
+            print(f"‚úó Test {test.__name__} failed with exception: {e}")
+
+    print(f"\nResults: {passed}/{total} tests passed")
+
+    if passed == total:
+        print("‚úÖ All corrected examples are working correctly!")
+        print("\nKey improvements:")
+        print("‚úÖ Uses correct LoggingSettings class")
+        print("‚úÖ Uses configure_logging() function")
+        print("‚úÖ Sinks configured as string names, not objects")
+        print("‚úÖ Environment variable configuration works")
+        print("‚úÖ Programmatic configuration works")
+        print("‚úÖ Multiple sinks configuration works")
+        return 0
+    else:
+        print("‚úó Some tests failed")
+        return 1
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/examples/test_fastapi_examples.py b/examples/test_fastapi_examples.py
new file mode 100644
index 0000000..0da244c
--- /dev/null
+++ b/examples/test_fastapi_examples.py
@@ -0,0 +1,145 @@
+#!/usr/bin/env python3
+"""
+Test script to verify FastAPI examples work correctly.
+This script tests the FastAPI app creation and basic functionality
+without requiring uvicorn to be installed.
+"""
+
+import sys
+import os
+
+# Add the src directory to the path
+sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))
+
+
+def test_basic_fastapi():
+    """Test the basic FastAPI example."""
+    print("Testing basic FastAPI example...")
+
+    try:
+        import importlib.util
+
+        spec = importlib.util.spec_from_file_location(
+            "basic_fastapi", "examples/05_fastapi_basic.py"
+        )
+        basic_module = importlib.util.module_from_spec(spec)
+        spec.loader.exec_module(basic_module)
+        app = basic_module.app
+
+        print("‚úÖ Basic FastAPI app created successfully")
+        print(f"‚úÖ App title: {app.title}")
+        print(f"‚úÖ App version: {app.version}")
+        print(f"‚úÖ Routes count: {len(app.routes)}")
+
+        return True
+    except Exception as e:
+        print(f"‚ùå Basic FastAPI example failed: {e}")
+        return False
+
+
+def test_middleware_fastapi():
+    """Test the middleware FastAPI example."""
+    print("\nTesting middleware FastAPI example...")
+
+    try:
+        import importlib.util
+
+        spec = importlib.util.spec_from_file_location(
+            "middleware_fastapi", "examples/06_fastapi_middleware.py"
+        )
+        middleware_module = importlib.util.module_from_spec(spec)
+        spec.loader.exec_module(middleware_module)
+        app = middleware_module.app
+
+        print("‚úÖ Middleware FastAPI app created successfully")
+        print(f"‚úÖ App title: {app.title}")
+        print(f"‚úÖ App version: {app.version}")
+        print(f"‚úÖ Routes count: {len(app.routes)}")
+
+        return True
+    except Exception as e:
+        print(f"‚ùå Middleware FastAPI example failed: {e}")
+        return False
+
+
+def test_error_handling_fastapi():
+    """Test the error handling FastAPI example."""
+    print("\nTesting error handling FastAPI example...")
+
+    try:
+        import importlib.util
+
+        spec = importlib.util.spec_from_file_location(
+            "error_handling_fastapi", "examples/07_fastapi_error_handling.py"
+        )
+        error_module = importlib.util.module_from_spec(spec)
+        spec.loader.exec_module(error_module)
+        app = error_module.app
+
+        print("‚úÖ Error handling FastAPI app created successfully")
+        print(f"‚úÖ App title: {app.title}")
+        print(f"‚úÖ App version: {app.version}")
+        print(f"‚úÖ Routes count: {len(app.routes)}")
+
+        return True
+    except Exception as e:
+        print(f"‚ùå Error handling FastAPI example failed: {e}")
+        return False
+
+
+def test_structured_logging_fastapi():
+    """Test the structured logging FastAPI example."""
+    print("\nTesting structured logging FastAPI example...")
+
+    try:
+        import importlib.util
+
+        spec = importlib.util.spec_from_file_location(
+            "structured_logging_fastapi", "examples/08_fastapi_structured_logging.py"
+        )
+        structured_module = importlib.util.module_from_spec(spec)
+        spec.loader.exec_module(structured_module)
+        app = structured_module.app
+
+        print("‚úÖ Structured logging FastAPI app created successfully")
+        print(f"‚úÖ App title: {app.title}")
+        print(f"‚úÖ App version: {app.version}")
+        print(f"‚úÖ Routes count: {len(app.routes)}")
+
+        return True
+    except Exception as e:
+        print(f"‚ùå Structured logging FastAPI example failed: {e}")
+        return False
+
+
+def main():
+    """Run all FastAPI example tests."""
+    print("=== FastAPI Examples Test ===")
+    print("Testing FastAPI integration examples...")
+    print()
+
+    results = []
+
+    results.append(test_basic_fastapi())
+    results.append(test_middleware_fastapi())
+    results.append(test_error_handling_fastapi())
+    results.append(test_structured_logging_fastapi())
+
+    print("\n" + "=" * 50)
+    print("Test Results:")
+    print(f"‚úÖ Passed: {sum(results)}")
+    print(f"‚ùå Failed: {len(results) - sum(results)}")
+
+    if all(results):
+        print("\nüéâ All FastAPI examples are working correctly!")
+        print("To run the servers, install uvicorn: pip install uvicorn")
+        print("Then run: uvicorn examples.05_fastapi_basic:app --reload")
+    else:
+        print("\n‚ö†Ô∏è  Some examples failed. Check the errors above.")
+
+    return all(results)
+
+
+if __name__ == "__main__":
+    success = main()
+    sys.exit(0 if success else 1)
diff --git a/examples/test_sinks_examples.py b/examples/test_sinks_examples.py
new file mode 100644
index 0000000..be8790d
--- /dev/null
+++ b/examples/test_sinks_examples.py
@@ -0,0 +1,294 @@
+#!/usr/bin/env python3
+"""
+Test script for sink-related examples.
+
+This script tests the sink examples without requiring external services
+like Loki or custom APIs.
+"""
+
+import asyncio
+import json
+import sys
+import tempfile
+import os
+from pathlib import Path
+
+# Add the src directory to the path
+sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
+
+from fapilog import configure_logging
+from fapilog.settings import LoggingSettings
+from fapilog.sinks.stdout import StdoutSink
+
+
+def test_loki_sink_example():
+    """Test the Loki sink example configuration."""
+    print("Testing Loki sink example...")
+
+    try:
+        # Import the example
+        from examples import loki_sink_example
+
+        # Test that the settings can be created
+        settings = loki_sink_example.loki_settings
+        assert settings.level == "INFO"
+        assert settings.format == "json"
+        assert len(settings.sinks) == 1
+        assert settings.sinks[0].__class__.__name__ == "LokiSink"
+
+        print("‚úì Loki sink example configuration is valid")
+        return True
+
+    except Exception as e:
+        print(f"‚úó Loki sink example failed: {e}")
+        return False
+
+
+def test_multiple_sinks_example():
+    """Test the multiple sinks example."""
+    print("Testing multiple sinks example...")
+
+    try:
+        # Create a test version with stdout only
+        test_settings = LoggingSettings(
+            level="DEBUG",
+            format="json",
+            sinks=[
+                StdoutSink(),
+                StdoutSink(),
+            ],
+        )
+
+        logger = configure_logging(test_settings)
+
+        # Test logging
+        logger.info("Test multiple sinks", extra={"test": True})
+        logger.debug("Debug message", extra={"level": "debug"})
+        logger.warning("Warning message", extra={"level": "warning"})
+
+        print("‚úì Multiple sinks example works")
+        return True
+
+    except Exception as e:
+        print(f"‚úó Multiple sinks example failed: {e}")
+        return False
+
+
+def test_custom_sink_example():
+    """Test the custom sink example."""
+    print("Testing custom sink example...")
+
+    try:
+        # Test with stdout sink instead of custom API
+        test_settings = LoggingSettings(
+            level="INFO",
+            format="json",
+            sinks=[
+                StdoutSink(),
+            ],
+        )
+
+        logger = configure_logging(test_settings)
+
+        # Test audit logging
+        logger.info(
+            "User action audit",
+            extra={
+                "log_type": "audit",
+                "user_id": "test-user",
+                "action": "test-action",
+                "resource": "test-resource",
+                "details": {"test": True},
+                "ip_address": "127.0.0.1",
+                "session_id": "test-session",
+            },
+        )
+
+        # Test regular logging
+        logger.info(
+            "Regular application log",
+            extra={
+                "user_id": "test-user",
+                "action": "test-action",
+                "resource": "test-resource",
+            },
+        )
+
+        print("‚úì Custom sink example works")
+        return True
+
+    except Exception as e:
+        print(f"‚úó Custom sink example failed: {e}")
+        return False
+
+
+def test_security_logging_example():
+    """Test the security logging example."""
+    print("Testing security logging example...")
+
+    try:
+        # Test security utilities
+        from examples.security_logging_example import (
+            mask_sensitive_data,
+            hash_sensitive_data,
+            sanitize_log_data,
+        )
+
+        # Test masking
+        masked_email = mask_sensitive_data("user@example.com")
+        assert masked_email == "us********@example.com"
+
+        # Test hashing
+        hashed_data = hash_sensitive_data("sensitive-data")
+        assert len(hashed_data) == 16
+
+        # Test sanitization
+        test_data = {
+            "email": "user@example.com",
+            "password": "secret123",
+            "credit_card": "1234567890123456",
+            "normal_field": "normal-value",
+        }
+        sanitized = sanitize_log_data(test_data)
+        assert sanitized["email"] == "us********@example.com"
+        assert sanitized["password"] == "se******23"
+        assert sanitized["credit_card"] == "12************56"
+        assert sanitized["normal_field"] == "normal-value"
+
+        # Test logging
+        test_settings = LoggingSettings(
+            level="INFO", format="json", sinks=[StdoutSink()]
+        )
+
+        logger = configure_logging(test_settings)
+
+        # Test security event logging
+        logger.info(
+            "Security event",
+            extra={
+                "log_type": "security",
+                "event_type": "login_attempt",
+                "email": masked_email,
+                "ip_address": "192.168.1.100",
+                "timestamp": "2024-01-15T10:30:00Z",
+            },
+        )
+
+        print("‚úì Security logging example works")
+        return True
+
+    except Exception as e:
+        print(f"‚úó Security logging example failed: {e}")
+        return False
+
+
+async def test_async_logging():
+    """Test async logging functionality."""
+    print("Testing async logging...")
+
+    try:
+        settings = LoggingSettings(level="INFO", format="json", sinks=[StdoutSink()])
+
+        logger = configure_logging(settings)
+
+        # Test async logging
+        async def async_function():
+            logger.info("Async function started")
+            await asyncio.sleep(0.1)
+            logger.info("Async function completed")
+
+        await async_function()
+
+        print("‚úì Async logging works")
+        return True
+
+    except Exception as e:
+        print(f"‚úó Async logging failed: {e}")
+        return False
+
+
+def test_file_logging():
+    """Test file logging functionality."""
+    print("Testing file logging...")
+
+    try:
+        # Create temporary file
+        with tempfile.NamedTemporaryFile(delete=False, suffix=".log") as f:
+            log_file_path = f.name
+
+        try:
+            settings = LoggingSettings(
+                level="DEBUG",
+                format="json",
+                sinks=[StdoutSink(file_path=log_file_path)],
+            )
+
+            logger = configure_logging(settings)
+
+            # Test logging to file
+            logger.info("Test file logging", extra={"test": True})
+            logger.debug("Debug message", extra={"level": "debug"})
+
+            # Check if file was created and contains logs
+            with open(log_file_path, "r") as f:
+                content = f.read()
+                assert "Test file logging" in content
+                assert "Debug message" in content
+
+            print("‚úì File logging works")
+            return True
+
+        finally:
+            # Clean up
+            try:
+                os.unlink(log_file_path)
+            except OSError:
+                pass
+
+    except Exception as e:
+        print(f"‚úó File logging failed: {e}")
+        return False
+
+
+def main():
+    """Run all sink example tests."""
+    print("Testing sink-related examples...\n")
+
+    tests = [
+        test_loki_sink_example,
+        test_multiple_sinks_example,
+        test_custom_sink_example,
+        test_security_logging_example,
+        test_file_logging,
+    ]
+
+    passed = 0
+    total = len(tests)
+
+    for test in tests:
+        try:
+            if test():
+                passed += 1
+        except Exception as e:
+            print(f"‚úó Test {test.__name__} failed with exception: {e}")
+
+    # Test async logging separately
+    try:
+        if asyncio.run(test_async_logging()):
+            passed += 1
+        total += 1
+    except Exception as e:
+        print(f"‚úó Async logging test failed: {e}")
+
+    print(f"\nResults: {passed}/{total} tests passed")
+
+    if passed == total:
+        print("‚úì All sink examples are working correctly!")
+        return 0
+    else:
+        print("‚úó Some tests failed")
+        return 1
+
+
+if __name__ == "__main__":
+    sys.exit(main())
